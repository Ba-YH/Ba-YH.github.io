<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>语言模型技术演讲综述：从N-gram到BERT</title>
      <link href="/2025/05/04/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B%E7%BB%BC%E8%BF%B0/"/>
      <url>/2025/05/04/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B%E7%BB%BC%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>语言模型是一种通过学习大量文本数据来理解、生成或预测自然语言的模型。核心目标是学习语言的结构和模式，并根据上下文预测下一个单词或生成合理的文本内容。通俗来讲，就是计算词序列（短语、句子、段落）概率分布的一种模型，这个概率表明了这句话的合理程度，即符合人类语言规则的程度。</p><p>用数学语言描述：给定一$n$个词组成的句子$S&#x3D;w_1,w_2,\dots,w_n$，根据上下计算下一个词的概率$P(w_n|w_1,w_2,\dots,w_{n-1})$，或计算整个句子的概率$P(S)&#x3D;P(w_1,w_2,\dots,w_n)$，$P(S)$表示语言中句子的分布概率。</p><p>语言模型的发展历程可以清晰地划分为三个主要阶段：早期的统计语言模型、随后出现的神经网络语言模型，以及今年来基于Tansformer架构的预训练语言模型。</p><h3 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a>统计语言模型</h3><p><a href="https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=15124">统计语言模型综述——论文地址</a></p><p>利用大型计算机和大规模的文本语料库进行<strong>统计建模</strong>，分析词语之间的搭配和出现频率，从而推导出词语的概率分布。</p><p>基本思想就是计算<strong>条件概率</strong>，利用条件概率乘法公式的推广，将句子的分布概率$P(S)$进行如下表示<br>$$<br>\begin{aligned}<br>P(S)&amp;&#x3D;P(w_1,w_2,\dots,w_n)&#x3D;P(w_1)P(w_2|w_1)P(w_3|w_2,w_1)\dots P(w_n|w_1,w_2,\dots w_{n-1}) \<br>&amp;&#x3D;\prod P(w_i|w_1,w_2,\dots,w_{i-1})<br>\end{aligned}<br>$$<br>一个句子理解成$n$个单词出现的组合，且第$i$个单词依赖于前$i-1$个单词。根据语料库中词语之间的出现频率，对下一个词的预测可以表示为<br>$$<br>P(w_i|w_1,w_2,\dots,w_i-1)&#x3D;\frac{count(w_1,w_2,w_3,\dots,w_i)}{count(w_1,w_2,\dots,w_{i-1})}<br>$$<br>但是，随着词汇量的增大，要想计算各词语之间的出现频率十分复杂，于是乎演化出了<strong>N-gram语言模型</strong>。</p><h4 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h4><p>引入马尔可夫假设，一阶的马尔可夫模型认为任意一个词$w_i$的出现概率仅与前一个词$w_{i-1}$相关。推广到N-1阶马尔科夫链的统计语言模型，假设当前词的概率只与其前N-1个连续的词相关，即N-gram模型。使用该模型对$P(S)$重新表示如下：<br>$$<br>P(S)&#x3D;P(w_1,w_2,\dots,w_n)&#x3D;\prod P(w_i|w_{i-(N-1)},w_{i-(N-2)},\dots,w_{i-1})<br>$$<br>当N&#x3D;2时，称为二元语言模型，公式计算会变得简单很多，即只统计每个词之后最有可能生成什么词<br>$$<br>P(w_i|w_{i-1})&#x3D;\frac{count(w_i,w_{i-1})}{count(w_i)}<br>$$<br>这和马尔可夫链用于文本生成任务的过程非常类似。实际上，如果将词作为状态，词与词之间的转移概率作为马尔可夫链的转移概率，那么二者在生成任务上是本质相同的。关于马尔可夫链的更多知识，详见<a href="https://blog.bayh.top/2025/04/28/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/">这篇文章</a>。</p><h5 id="平滑技术"><a href="#平滑技术" class="headerlink" title="平滑技术"></a>平滑技术</h5><blockquote><p><strong>数据稀疏问题</strong>：统计语言模型中，训练数据中某些词或短语可能从未出现，或其上下文信息不足，导致模型在估计这些词序列概率时可能出现零概率问题。根据乘法原理，一个词的分布概率为0时，整个句子概率也为0。</p></blockquote><p>平滑技术就时为那些在训练数据中未出现或出现次数极少的单词或短语提供一个<strong>非零的概率估计</strong>，从而使模型预测更为合理。</p><p>加K平滑策略，加1平滑的泛化形式，相当于给每个统计单元与预设一个初始值K<br>$$<br>P(w_i|w_{i-1})&#x3D;\frac{count(w_i,w_{i-1})+K}{count(w_i)+K\times|V|}<br>$$</p><h6 id="古德-图灵平滑"><a href="#古德-图灵平滑" class="headerlink" title="古德-图灵平滑"></a>古德-图灵平滑</h6><p>将出现次数为r的事件的频率，用出现次数为r+1的事件的频率来估计，合理的调整低频事件概率分布<br>$$<br>\begin{aligned}<br>&amp;r^<em>&#x3D;\frac{(r+1)\cdot N_{r+1}}{N_r} \<br>&amp;P(w)&#x3D;\frac{r^</em>}{N}<br>\end{aligned}<br>$$</p><h6 id="回退平滑"><a href="#回退平滑" class="headerlink" title="回退平滑"></a>回退平滑</h6><p>当高阶N-gram数据不足时，回退到低阶来估计概率。下面为K回退的计算公式，结合回退机制和古德-图灵估计，对高低频采取不同的处理方式。<br>$$<br>P(w_n|w_{n-2},w_{n-1})&#x3D;\begin{cases}d_r\cdot\frac{C(w_{n-2},w_{n-1},w_n)}{C(w_{n-2},w_{n-1})},&amp;\mathrm{if~}C&gt;k\\alpha(w_{n-2},w_{n-1})\cdot P(w_n|w_{n-1}),&amp;\mathrm{otherwise}&amp;\end{cases}<br>$$<br>其中，$d_r$是折扣值，$\alpha$是回退权重</p><h3 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h3><p>虽然使用平滑技术能够使得统计语言模型正常工作，但仍存在一些问题</p><ul><li><p>只考虑短距离的上下文大幅度简化了计算，但也无法捕捉长距离的语义和语法依赖</p></li><li><p>随着词汇量和阶数的增加，模型参数规模呈指数级增长。一个词汇量为$V$的$n$元语言模型需要$V^n-1$个参数</p></li><li><p>基于词的表面共现统计，难以捕捉词与词之间的深层语义关系</p></li></ul><p>鉴于上面的问题，人们开始尝试使用神经网络建立语言模型，通过词向量的距离衡量单词之间的相似度，进而避免出现数据稀疏问题。根据神经网络的种类，又可以被分为前馈神经网络语言模型（FFNNLM）和循环神经网络语言模型（RNNLM）</p><h4 id="前馈神经网络语言模型"><a href="#前馈神经网络语言模型" class="headerlink" title="前馈神经网络语言模型"></a>前馈神经网络语言模型</h4><p><a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>该论文提出了一个非常经典的神经网络语言模型，模型的架构图如下</p><img src="https://files.catbox.moe/k489mq.png" alt="k489mq.png" style="zoom: 67%;" /><h5 id="模型拆解"><a href="#模型拆解" class="headerlink" title="模型拆解"></a>模型拆解</h5><p>实际上，这个2003年提出的模型非常简单。和N-gram一样，NNLM也假设当前词仅依赖于前N-1个词。总体上看，将$w_i$前$n-1$个词的词向量进行拼接作为网络输入，经过一次非线性变换，最后输出字典中每个词的概率作为预测结果。<br>$$<br>\begin{aligned}<br>&amp;x&#x3D;(C(w_{t-1},C(W_{t-2}),\dots,C(W_{t-n+1})) \<br>&amp;y&#x3D;b+W\cdot x+U\cdot tanh(d+H\cdot x) \<br>&amp;p(w_i|w_{t-1},\ldots,w_{t-n+1})&#x3D;\frac{e^{y_{w_t}}}{\sum_ie^{y_i}}<br>\end{aligned}<br>$$</p><h6 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h6><p>模型训练时，需要最大化下列式子。第一部分是最大化给定序列$w_1^{t-1}$，下一个词为$w_t$的概率，第二部分$R(\theta)$为正则项。<br>$$<br>L&#x3D;\frac{1}{T}\sum_tlogf(w_t,w_{t-1},\ldots,w_{t-n+1};\theta)+R(\theta)<br>$$<br>参数优化方法使用梯度下降法。</p><h5 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h5><p>实验结果表明，这个结构在现在看来非常简单的NNLM模型比平滑处理的三元语言模型表现更好。模型虽然简单，但作为神经网络语言模型的开山之作，让人们意识到了神经网络语言模型的威力，逐渐进入了学界主流。</p><p>此外，该模型同时产生了副产品——词向量。</p><p>在表示输入层时，用到的映射函数$C(i)\in R^m$，其实就是词的分布式表示。$C$是一个$|V|\times m$的矩阵，$m$为词向量维度，矩阵的每一行就是一个单词的词向量。在模型训练过程中逐渐<strong>学习</strong>到词的分布式表示。可以说，后来的<strong>word2vec</strong>模型就是对FFNNML的<strong>简化和优化</strong>，且专注于词向量的训练。</p><h6 id="word2vec-Vs-FFNNLM"><a href="#word2vec-Vs-FFNNLM" class="headerlink" title="word2vec Vs FFNNLM"></a>word2vec Vs FFNNLM</h6><ul><li>FFNNLM是完整的语言模型，它的目标是最大化下一个词的预测概率，只是顺带学习了词向量。</li><li>是剥离出来的词向量学习模块，目标就是学习词向量，利用上下文预测中心词或者中心词预测上下文的形式，不建模完整的概率分布，通过大量的优化使得词向量训练变得高效、可扩展、泛用性强。</li></ul><p>word2vec的两个具体模型和优化方法详见<a href="https://blog.bayh.top/2025/04/27/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/">文本数据处理方法</a>这篇文章。</p><h4 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h4><p>从方法创新的角度上看，从FFNNLM到RNNLM，只是网络结构上的过渡：用RNN&#x2F;LSTM替换了FFNN。所以，在这里有必要先介绍一下循环神经网络。</p><h5 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h5><p>在传统的前馈神经网路中，每个输入与输出之间的关系是独立的，即网络的输入和输出之间没有时间上的依赖关系。然而，在许多现实世界的任务中，数据往往是序列化的，即数据之间存在时间上的顺序关系。比如，NLP中的文本数据、语音识别中的音频数据、股票价格的时间序列数据等，都具有明显的时序性。</p><p>为了解决这一问题，循环神经网络RNN出场了。下图是RNN的循环示意图，同时接受当前时刻的输入和上一时刻隐藏状态，再由此计算出当前时刻的隐藏状态，供下一时刻使用 </p><p><img src="https://files.catbox.moe/k9ticb.png" alt="k9ticb.png"></p><p>采用$tanh$作为激活函数，其中$W_h$为层级之间的权重参数，$W_x$为当前输入的权重参数。<br>$$<br>h_t&#x3D;tanh(h_{t-1}W_h+x_tW_x+b)<br>$$</p><h6 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h6><p>出于减少训练时间或者简化模型的目的，层级之间的权重参数通常是相同的。在从后往前进行反向传播时，随着权重参数以指数形式进行积累，梯度传递到靠前的时间步时可能会变得非常小或者非常大，难以维持有效的学习过程。正是因为梯度消失的问题，RNN难以保持长期依赖性，因为在传播过程中，远期信息相关梯度会在传播过程中消失。</p><p>用数学语言描述，损失函数$L$对权重参数通过时间步进行反向传播<br>$$<br>\begin{aligned}<br>\frac{\partial L}{\partial W_h}&#x3D;\sum_{t&#x3D;1}^T\frac{\partial L}{\partial h_t}\cdot\frac{\partial h_t}{\partial W_h}\</p><p>\end{aligned}<br>$$<br>在RNN中，$\frac{\partial L}{\partial h_t}$ 依赖于所有未来时刻的隐藏状态，因此需要通过链式法则进行传播：<br>$$<br>\begin{aligned}<br>\frac{\partial L}{\partial h_t}&amp;&#x3D;\frac{\partial L}{\partial h_T}\cdot\prod_{k&#x3D;t}^{T-1}\frac{\partial h_{k+1}}{\partial h_k} \<br>&amp;&#x3D;\frac{\partial L}{\partial h_T}\cdot\prod_{k&#x3D;t}^{T-1}\left(\phi^{\prime}(a_k)\cdot W_h\right)<br>\end{aligned}<br>$$<br>其中 $a_k &#x3D; W_{h} h_{k} + \cdots$ 是激活函数的输入。</p><p>观察最右边的乘积项，由于这是矩阵和标量导数的乘积，如果：</p><ul><li>$|\phi’(a_k) \cdot W_{h}| &lt; 1$，则梯度会随着时间步数增长而指数级减小，导致<strong>梯度消失</strong>；</li><li>$|\phi’(a_k) \cdot W_{h}| &gt; 1$，则梯度会指数级增长，导致<strong>梯度爆炸</strong>。</li></ul><p>为了解决RNN由于梯度消失造成的难以捕捉长期依赖的问题，长短期记忆网络LSTM出现了。 </p><h5 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h5><p><a href="https://www.bilibili.com/video/BV1Z34y1k7mc?spm_id_from=333.788.videopod.sections&vd_source=3464224b3cb14e06ba5a481bb86a2285">视频讲解</a></p><p>全称为Long Short-Term Memory 长短期记忆网络，引入门控机制控制信息流动，更好的捕捉长期依赖关系。</p><p>为了方便理解，我们可以将传统 RNN 中的隐藏状态序列看作一条<strong>短期记忆链</strong>，它在时间上传递并更新当前的上下文信息。而 LSTM 在此基础上引入了另一条名为“细胞状态”的信息链条，可类比为<strong>长期记忆链</strong>。</p><p>在每个时间步，LSTM 通过门控机制（包括输入门、遗忘门和输出门）对长期记忆链上的信息进行选择性更新，从而实现对长期依赖信息的保留与调整。这两条信息链（隐藏状态与细胞状态）在每个时间步协同作用，实现了对短期与长期信息的有效建模。</p><p><img src="https://files.catbox.moe/je0gpg.png" alt="je0gpg.png"></p><p>其中$W_{f,i,o,c}$和$U_{f,i,o,c}$均为权重矩阵，三个门都只依赖当前输入和上一时刻隐藏状态，遗忘门和输入门来更新记忆单元，输出门控制信息从记忆单元流向隐藏状态。<br>$$<br>\begin{aligned}<br>&amp;f_{t}&#x3D;\sigma(W_fx_t+U_fh_{t-1}+b_f)&amp;&amp;\text{遗忘门}\<br>&amp;i_{t}&#x3D;\sigma(W_ix_t+U_ih_{t-1}+b_i)&amp;&amp;\text{输入门}\<br>&amp;o_{t}&#x3D;\sigma(W_ox_t+U_oh_{t-1}+b_o)&amp;&amp;\text{输出门}\<br>&amp;\tilde{c}<em>{t}&#x3D;\tanh(W_cx_t+U_ch</em>{t-1}+b_c)&amp;&amp;\text{候选伏态}\<br>&amp;c_{t}&#x3D;f_t\odot c_{t-1}+i_t\odot\bar{c}<em>t&amp;&amp;\text{记忆单元更新}\<br>&amp;h</em>{t}&#x3D;o_t\odot\tanh(c_t)&amp;&amp;\text{隐藏状态}<br>\end{aligned}<br>$$</p><h6 id="如何缓解梯度问题"><a href="#如何缓解梯度问题" class="headerlink" title="如何缓解梯度问题"></a>如何缓解梯度问题</h6><p>观察记忆状态更新公式：<br>$$<br>c_{t}&#x3D;f_t\odot c_{t-1}+i_t\odot\bar{c}<em>t<br>$$<br>在进行反向传播时：<br>$$<br>\begin{aligned}<br>&amp;\frac{\partial C_k}{\partial C</em>{k-1}}&#x3D;f_k+other\<br>&amp;\prod_{k&#x3D;t+1}^T\frac{\partial C_k}{\partial C_{k-1}}&#x3D;f_kf_{k+1}\ldots f_T+other\end{aligned}<br>$$<br>虽然在形式上依然是一个连乘式，但这里的$f_k$是可训练学习的，不会像RNN一样，每一步始终小于1或者大于1。所以整体也就不会一直减小，远距离梯度不至于完全消失，解决了RNN中梯度消失的问题，更好的捕捉长期依赖。</p><h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><p>是对传统RNN的改进，旨在缓解长序列训练中常见的梯度消失问题，同时比LSTM结构更简单、计算成本更低。</p><p>在GRU中，没有像LSTM新增的长期记忆链，而是直接将细胞状态和隐藏状态合并。此外，将遗忘门和输入门合并为更新门，并且同时充当输入门的作用，同时新增一个”重置门”，控制当前输入与过去隐藏状态的输入融合程度。这种结构设计使GRU在保持信息处理能力的同时，简化了模型，提高计算效率。<br>$$<br>\begin{aligned}<br>&amp;z_t&#x3D;\sigma(W_zx_t+U_zh_{t-1})&amp;&amp;\text{更新门}\<br>&amp;r_t&#x3D;\sigma(W_tx_t+U_rh_{t-1})&amp;&amp;\text{重置门} \<br>&amp;\tilde{h}<em>t&#x3D;\tanh(W_hx_t+U_h(r_t\odot h</em>{t-1}))&amp;&amp;\text{候选隐藏状态}\<br>&amp;h_t&#x3D;(1-z_t)\odot h_{t-1}+z_t\odot\tilde{h}_t&amp;&amp;\text{最终隐藏状态}\<br>\end{aligned}<br>$$</p><h6 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h6><p>GRU 适用于各种需要处理序列数据或时间依赖的任务，尤其在<strong>数据量较小或对训练效率要求较高</strong>的场景中表现良好。</p><p>实际上，LSTM 复杂的原因，除了在结构上天然地克服了梯度消失的问题，更重要的是具有更多的参数来控制模型；通过四倍于RNN的参数量，可以更加精细地预测时间序列变量。 </p><h4 id="循环神经网络语言模型"><a href="#循环神经网络语言模型" class="headerlink" title="循环神经网络语言模型"></a>循环神经网络语言模型</h4><p>论文地址 <a href="https://www.fit.vut.cz/research/group/speech/public/publi/2010/mikolov_interspeech2010_IS100722.pdf">Recurrent neural network based language model</a></p><h5 id="模型拆解-1"><a href="#模型拆解-1" class="headerlink" title="模型拆解"></a>模型拆解</h5><p>给定上下文$s(t-1)$和当前词$w(t)$的情况下，预测下一个词为词汇表中第$k$个词的概率$y_k(t)$:<br>$$<br>\begin{aligned}<br>&amp;y_k(t)&#x3D;g(\sum_js_j(t)v_{kj}) \<br>&amp;\sum_ky_k(t)&#x3D;1<br>\end{aligned}<br>$$<br>$s_j(t)$是隐藏层的输出，$v_{kj}$是隐藏层到输出层的权重矩阵元素，$g$为Softmax函数</p><h6 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h6><p>RNNLM的目标是使预测的概率分布$y(t)$尽可能接近真实的目标词的分布。</p><p>交叉熵损失函数衡量预测分布和真实分布之间的差异。对于单个样本和整个训练数据集，损失函数定义为：<br>$$<br>\begin{aligned}<br>&amp;L(t)&#x3D;-\sum_k{d_k(t)log(y_k(t))} \<br>&amp;L&#x3D;-\frac{1}{T}\sum_{t&#x3D;1}^TL(t)<br>\end{aligned}<br>$$</p><h3 id="序列到序列模型"><a href="#序列到序列模型" class="headerlink" title="序列到序列模型"></a>序列到序列模型</h3><p>序列到序列模型（Seq2Seq）是神经网络语言模型的一种，广泛应用于机器翻译、文本摘要、对话生成等任务。它的核心思想是将一个输入序列编码成一个语义表示，然后再由解码器将这个表示转换为目标序列。这个过程体现了语言模型的本质：根据已有的信息预测合理的语言输出。</p><p>用数学语言描述，给定一个源语言序列$X$，训练模型预测目标序列$Y&#x3D;(y_1,y_2,\dots,y_T)$，让模型学习：<br>$$<br>P(Y\mid X)&#x3D;P(y_1\mid X)\cdot P(y_2\mid y_1,X)\cdots\cdots P(y_T\mid y_1,…,y_{T-1},X)<br>$$<br>在Seq2Seq模型中，我们可能会看到$ P(y_t\mid y_{&lt;t},X)$ 这样的表达式，其实就表示模型在看到输入$X$和目标前缀$y_{&lt;t}$后，预测下一个词$y_t$的概率分布，取其中$y_t$的概率。</p><h4 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h4><p>下图为编码器-解码器模型的结构，先对句子$X(x_1,x_2,x_3,x_4)$编码，然后再解码为输出$Y(y_1,y_2,y_3)$，就能实现机器翻译和对话生成等热任务。</p><p><img src="https://files.catbox.moe/gxyjt8.png" alt="gxyjt8.png"></p><p>Seq2Seq 最早是基于循环神经网络构建的，即编码器和解码器的核心结构都是RNN，后来演变出使用LSTM、GRU等改进结构来解决长距离依赖的问题。后来，为了提高效果，还加入了注意力机制，并最终发展成Transformer架构，基于注意力机制的序列到序列模型成为主流。</p><p>无论使用哪种结构，其本质都是利用神经网络对语言进行建模，因此属于神经网络语言模型的范畴。它相比传统的语言模型更强大，能够处理输入输出长度不等的复杂语言任务。</p><p>为了讲清楚Transformer这一架构，我们先从Attention机制开始说起。</p><h4 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h4><p><a href="https://www.bilibili.com/video/BV1xS4y1k7tn?spm_id_from=333.788.videopod.sections&vd_source=3464224b3cb14e06ba5a481bb86a2285">注意力机制-视频讲解</a></p><h5 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h5><p>观察上图中Encoder-Decoder结构，可以发现如下问题：</p><ul><li>编码器将整个输入序列压缩为一个固定长度的上下文向量，可能导致信息丢失，尤其是在处理长序列时。</li><li>解码器只能依靠单一的上下文向量来生成整个输出序列，缺乏对输入序列中具体位置的关注能力。</li></ul><p><a href="https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5">Neural Machine Translation by Jointly Learning to Align and Translate</a> Attention机制的开山之作，首次引入Attention机制，旨在解决传统编码器-解码器（Encoder-Decoder）模型在处理长句子时的性能下降问题。作者提出了一种新的神经机器翻译模型，该模型在解码每个词时，通过注意力机制自动对源句子的不同部分进行加权，从而实现对源句子的动态对齐和翻译。这种机制允许模型在生成每个目标词时，关注源句子中最相关的部分，从而提高翻译质量。</p><p><img src="https://files.catbox.moe/jx85vk.png" alt="jx85vk.png"></p><h5 id="Additive-Attention"><a href="#Additive-Attention" class="headerlink" title="Additive Attention"></a>Additive Attention</h5><p>加性注意力机制</p><ol><li><p>输入经过双向RNN编码，得到隐藏状态序列$h&#x3D;{h_1,h_2,\dots,h_T}$</p></li><li><p>对每个目标词计算源词的对齐程度，并归一化注意力分数<br> $$<br> \begin{aligned}<br> &amp;e_{ti}&#x3D;v_a^T\tanh(W_as_{t-1}+U_ah_i)\<br> &amp;a_{ti}&#x3D;\frac{exp(e_{ti})}{\sum_{k&#x3D;1}^Texp(e_{tk})}<br> \end{aligned}<br> $$</p><p> $s_{t-1}$为前一个时间步的解码器状态，其余为可学习的参数矩阵</p></li><li><p>计算上下文向量并生成目标词<br>$$<br>\begin{aligned}<br> &amp;c_t&#x3D;\sum_{i&#x3D;1}^Ta_{ti}h_i\<br> &amp;s_t&#x3D;f(s_{t-1},y_{t-1},c_t)\<br> &amp;P(y_t|y_{&lt;t},x)&#x3D;g(s_t,c_t)<br> \end{aligned}<br>$$</p></li></ol><p>模型不再压缩为一个固定向量，而是为每个目标词选择性地关注源句的不同部分。</p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><p>尽管引入Attention机制显著提升了模型性能，但当时的架构仍依赖于复杂的递归神经网络。由于RNN在计算上存在时间步之间的依赖，导致模型难以实现并行计算，限制了训练效率。为了解决这一问题，研究者们提出了一种完全基于注意力机制的全新架构 —— Transformer，彻底摒弃了RNN结构，从而显著提升了并行处理能力与训练效率。</p><p><a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention Is All You Need</a> Transformer的开山之作，在语言模型的发展史上具有划时代的意义。</p><p>在介绍transformer结构之前，我们先讲清楚几个关键技术。</p><h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>由于出于对并行计算效率的考虑，Transformer 模型完全摒弃了传统的循环神经网络（RNN）结构。然而，RNN 通过其递归结构天然地捕获了序列中各个词语的顺序信息，而这类顺序信息在自然语言理解中是至关重要的。如果完全忽略词语的位置信息，模型将无法区分语序不同但词汇相同的句子，从而导致语义理解偏差。</p><p>为了解决这一问题，引入了位置编码（Postion Encoding)机制，用以显式地为模型提供每个词在序列中的相对或绝对位置信息。这些位置信息被表示为向量，并与词嵌入（Word Embedding）具有相同的维度，从而可以通过简单的向量加法将两者融合。</p><blockquote><p>思考：为什么可以直接简单相加？</p><p>词向量和位置向量记录的是两类本质上完全不同的信息：前者捕捉的是词语的语义信息，而后者则编码词语在序列中的位置信息。尽管我们将它们简单相加作为模型输入，但这一操作背后的合理性目前并没有严格的数学理论支撑。我们只是<strong>经验性地认为</strong>，在后续深层神经网络的训练过程中，模型可以“自动”从这些融合后的数值中<strong>学习出隐含的结构和规律</strong>，进而完成有效的语言理解。</p><p>这种“结果导向”的思维方式，正体现了近年来人工智能领域的一个重要趋势转变。实际上，从2012年AlexNet以压倒性优势赢得竞赛以来，深度学习模型的设计就逐渐从追求可解释性转向<strong>追求性能优先</strong>。在这一背景下，<strong>模型效果被置于首位</strong>，而对模型内部工作机制的可解释性探索则相对被弱化甚至忽略。</p></blockquote><h5 id="位置编码的计算"><a href="#位置编码的计算" class="headerlink" title="位置编码的计算"></a>位置编码的计算</h5><p>Transformer中的位置编码使用$sin$和$cos$交替的形式：<br>$$<br>\begin{aligned}<br>&amp;PE_{(pos,2i)}&#x3D;sin(\frac{pos}{10000^{2i&#x2F;d_{model}}}) \<br>&amp;PE_{(pos,2i+1)}&#x3D;cos(\frac{pos}{10000^{2i&#x2F;d_{model}}})<br>\end{aligned}<br>$$<br>$d_{model}$即为当前模型位置向量的维度</p><h5 id="重要性质"><a href="#重要性质" class="headerlink" title="重要性质"></a>重要性质</h5><p>观察上述的计算公式，位置编码本身是一个绝对位置信息，但在语言模型中，相对位置也很重要。<br>$$<br>\left.<br>\left{\begin{array}{l}sin(\alpha+\beta)&#x3D;sin\alpha cos\beta+cos\alpha sin\beta \<br>cos(\alpha+\beta)&#x3D;cos\alpha cos\beta-sin\alpha sin\beta\end{array}\right.<br>\right.<br>$$<br>根据上述的公式，当我们需要计算$pos+k$位置上的位置编码时，可以通过位置$pos$和$k$的编码表示：<br>$$<br>\left.<br>\left{\begin{array}{l}PE(pos+k,2i)&#x3D;PE(pos,2i)\times PE(k,2i+1)+PE(pos,2i+1)\times PE(k,2i)\<br>PE(pos+k,2i+1)&#x3D;PE(pos,2i+1)\times PE(k,2i+1)-PE(pos,2i)\times PE(k,2i)\end{array}\right.<br>\right.<br>$$<br>这意味着，<strong>两个位置编码之间的差异只与它们之间的距离有关，而与它们的具体位置无关</strong>。</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>Transformer 使用 sin 和 cos 交替编码，是一种<strong>无需训练却具有丰富结构的表示方式</strong>，它既能保留词的<strong>绝对位置信息</strong>，又能通过数学结构间接捕捉<strong>相对位置关系</strong>，从而弥补模型自身结构中缺乏顺序感的缺陷。</p><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>在上文的Attention部分，我们简单介绍了加性注意力进制的基本原理，$a_{ti}$表示当前位置$t$对于输入序列中地$i$位置的关注权重。而在Self-Attention机制中，模型会对输入序列中每个位置计算其对其他所有位置的注意力权重，从而实现信息的全局建模。</p><h5 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h5><ol><li><p>对于输入序列$S(w_1,w_2,\dots,w_n)$，首先构造$n\times d$的输入矩阵$X$，$X(i)&#x3D;WE(w_i)+PE(i)$</p></li><li><p>通过三个随机初始化的大小为$d\times d$的矩阵$W^Q,W^K,W^V$<br> $$<br> \begin{aligned}<br> &amp;Q&#x3D;XW^Q\<br> &amp;K&#x3D;XW^K\<br> &amp;V&#x3D;XW^V<br> \end{aligned}<br> $$<br> Q,K,V的形状为$n\times d$</p></li><li><p>使用点积相似度计算注意力分数<br> $$<br> Attention &#x3D; softmax(\frac{QK^T}{\sqrt{d_k}})V<br> $$</p></li></ol><h5 id="各矩阵的含义"><a href="#各矩阵的含义" class="headerlink" title="各矩阵的含义"></a>各矩阵的含义</h5><ul><li>Q 代表着需要编码的词的信息</li><li>K 代表着句子中其它词的信息，相乘后得到句子中其它词的权重值； </li><li>V 代表着每个位置单词蕴含的语义信息，在被加权求和后作为待编码词的编码。</li></ul><p>$QK^T$的形状为$n\times n$，就是一个word2word的attention map，每个词对应每个词都会有一个权重。</p><p>再与$V$相乘，根据矩阵的乘法规则。结果中的矩阵元素$A_{ij}$就代表单词$i$在对这个句子总所有词的关注度在语境$j$下的总和。<br>$$<br>A_{ij}&#x3D;\sum_l^d(QK^T)<em>{il}V</em>{lj}<br>$$</p><h4 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h4><h5 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h5><p>Mutil-head Attention就是Self-Attention的进阶版，并不是直接利用$d$维的Q,K,V矩阵计算，而是将其都划分为多头，并分别计算注意力，最后通过全连接层获得新的注意力值。</p><p>设有$h$个头，每个头的维度即为$d_h&#x3D;d&#x2F;h$，<br>$$<br>\begin{aligned}<br>&amp;Q_i&#x3D;XW_i^Q,\quad K_i&#x3D;XW_i^K,\quad V_i&#x3D;XW_i^V \<br>&amp;\text{head}_i&#x3D;\text{Attention}(Q_i,K_i,V_i)\<br>&amp;\text{MultiHead}(X)&#x3D;\text{Concat}(\text{head}_1,\text{head}_2,\dots,\text{head}_h)W^O<br>\end{aligned}<br>$$<br>Self-Attention的计算公式中的 ${d_k}$ 其实就表示的是每个头的维度，防止梯度过大导致训练不稳定。</p><p>至此，Transformer架构中注意力部分的计算的整个流程如下图所示。</p><p><img src="https://files.catbox.moe/2lvtbi.png" alt="2lvtbi.png"></p><h5 id="意义-1"><a href="#意义-1" class="headerlink" title="意义"></a>意义</h5><p>多个头用不同的参数训练，让模型在不同的注意力视角下捕捉不同的注意力的语义信息，增强最终表示的丰富性。</p><h4 id="Masked"><a href="#Masked" class="headerlink" title="Masked"></a>Masked</h4><p>在上文中提到，$QK^T$得到的是一个attention map，而掩码机制就是掩盖住矩阵的上三角区域，只保留下三角区域，具体做法就是在上三角区域加上一个很大的负数，使得经过Softmax后接近于0，相当于屏蔽</p><h5 id="意义-2"><a href="#意义-2" class="headerlink" title="意义"></a>意义</h5><p>防止模型看到未来信息。在序列生成任务时，不应该看到后面的词，否则模型在训练阶段可以利用未来消息进行预测。然而在测试阶段中，后面的词是不会对当前翻译结果产生贡献的。所以为了让训练和测试阶段的行为一致，避免过拟合</p><h4 id="Add-Norm"><a href="#Add-Norm" class="headerlink" title="Add &amp; Norm"></a>Add &amp; Norm</h4><p>从上图中的结构可以发现，<strong>Add &amp; Norm</strong>是在每个子层（自注意力子层，前馈子层）之后的一个标准步骤</p><p>假设一个子层的输入为$x$，输出是$SubLayer(x)$，那么输出<br>$$<br>\text{Output}&#x3D;\text{LayerNorm}(x+\text{SubLayer(x)})<br>$$<br>实际上包括了残差连接和层归一化两个步骤：<br>$$<br>\begin{aligned}<br>&amp;z&#x3D;x+\text{SubLayer}(x)&amp;&amp;\text{残差连接}\<br>&amp;\text{LayerNorm}(z)&#x3D;\frac{z-\mu}\sigma\cdot\gamma+\beta &amp;&amp;\text{层归一化}<br>\end{aligned}<br>$$<br>其中$\mu,\sigma$是均值和标准差，$\gamma,\beta$是可训练参数</p><h5 id="意义-3"><a href="#意义-3" class="headerlink" title="意义"></a>意义</h5><ul><li><p>残差连接：帮助训练更深的网络，避免梯度消失；保留原始输入的信息。</p><p>最终输出是“原始输入+学习到的偏差”</p><p>通过加上原始输出，在反向传播的梯度计算时候留一个恒等项$I$，即使$\frac{\partial\mathcal{F}(x)}{\partial x}$很小，仍能保证梯度不完全为0。<br>$$<br>\begin{aligned}<br>&amp;\mathrm{Output}&#x3D;x+\mathcal{F}(x)\<br>&amp;\frac{\partial\text{Output}}{\partial x}&#x3D;\frac{\partial(x+\mathcal{F}(x))}{\partial x}&#x3D;I+\frac{\partial\mathcal{F}(x)}{\partial x}<br>\end{aligned}<br>$$</p></li><li><p>层归一化：稳定训练，提高收敛速度；避免不同特征维度的分布不一致。</p></li></ul><h4 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h4><p>有了上面知识的铺垫，我们再来看论文中Transformer整体框架图。</p><img src="https://files.catbox.moe/fwu6us.png" alt="fwu6us.png" style="zoom: 50%;" /><p>实际上，在序列到序列模型部分，我们提到Transformer模型也是Seq2Seq模型，从输入输出角度来讲，就是通过Encoder将输入读进去，然后用Decoder得到输出。所以模型可以简化表示成如下形式：</p><p><img src="https://files.catbox.moe/c9mjn7.png" alt="c9mjn7.png"></p><p>编码部分有若干个Encoder组成，每个Encoder主要由多头注意力进制和前馈神经网络组成；解码部分也由若干Decoder组成，每个Decoder组成和Encoder类似，但多了一层Encoder-Decoder Attention，该层会利用编码部分的输出。接下来，我们继续拆解编码器结构，得到如下图所示的结构，下面依次对Encoder、Decoder、模型输出展开介绍，并模拟整个模型从输入到输出的数据流动</p><p><img src="https://files.catbox.moe/mj1r6q.png" alt="mj1r6q.png"></p><h5 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h5><ol><li>词向量加上位置向量作为编码器的输入$x$</li><li>经过多头自注意力进制计算后，得到注意力分数$z$</li><li>进行残差连接$x+z$和层归一化，得到输出$x’$</li><li>经过前馈神经网络层，包括两个线性变换和一个Relu激活函数，$FFN(x)&#x3D;max(0,xW_1+b_1)W_2+b2$</li><li>再经过Add &amp; Norm得到输出并作为下一个编码器的输入</li></ol><h5 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h5><p>和Encoder过程非常相似，值得注意的是Encoder-Decoder Attention。</p><p>并不是一种新的注意力进制，指的是利用Decoder提供的$Q_d$矩阵，和Encoder提供的$K_e,V_e$矩阵来计算注意力分数。和加性注意力进制一个道理，我们关注的还是<strong>输出位置对每个输入位置的注意力</strong>。</p><h5 id="模型输出"><a href="#模型输出" class="headerlink" title="模型输出"></a>模型输出</h5><p>解码器的输出是一个向量，经过liner+Softmax变为单词</p><p>liner：简单的全连接网络，把解码器输出投影为一个$|V|$维向量</p><p>softmax：进一步归一化，将分数转化为概率，再根据概率最大的索引查找关联词</p><h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>模型输出的是对每个token的预测向量，对于语言建模任务依然使用交叉熵误差：<br>$$<br>\mathcal{L}&#x3D;-\sum_t\log P(y_t\mid y_{&lt;t},X)<br>$$<br>使用梯度下降法进行参数优化。</p><h3 id="基于Trm的预训练语言模型"><a href="#基于Trm的预训练语言模型" class="headerlink" title="基于Trm的预训练语言模型"></a>基于Trm的预训练语言模型</h3><h4 id="预训练语言模型"><a href="#预训练语言模型" class="headerlink" title="预训练语言模型"></a>预训练语言模型</h4><h5 id="预训练"><a href="#预训练" class="headerlink" title="预训练"></a>预训练</h5><p>在深度学习与自然语言处理的交汇点上，”预训练”这一概念彻底改变了我们构建语言模型的方式。预训练本质上是一种知识迁移的范式，其核心思想是让模型在海量未标注文本中自主学习语言的通用规律，而非直接针对特定任务进行训练。具体来说，先在大规模数据集上训练模型，让它学习通用的特征和知识，再将其应用到具体任务中进行进一步训练。这种先验知识的积累使模型摆脱了传统监督学习对标注数据的依赖，为后续任务适配提供了可迁移的知识基础。</p><h5 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h5><p>预训练模型是预训练这一过程的具象化产物。更具体的说，是经过预训练阶段训练完成、已经掌握了通用语言能力的模型，从而可以用于下游任务。</p><p>在<a href="https://blog.bayh.top/2025/04/27/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/">文本数据处理方法</a>一文中我提到的<strong>词向量迁移学习</strong>，本质上也是一种预训练的雏形。因为训练好的词向量是捕获了词的语义和语法关系，这些知识是通用的，可用于文本分类、机器翻译等多种下游任务，无需重新训练。所以，word2vec、Glove可以看作是<strong>预训练词向量模型</strong>，通过浅层网络学习静态词嵌入，为后续任务提供基础语义单元。</p><p>预训练语言模型以<strong>自然语言文本数据</strong>为训练对象，是预训练技术在自然语言处理领域的具体实现。</p><h5 id="使用模式"><a href="#使用模式" class="headerlink" title="使用模式"></a>使用模式</h5><p>预训练模型下游使用的方法可以分为几种主要策略</p><ol><li><p>预训练+微调（Fine-tuning)</p><p> 最主流的方法，在具体下游任务的数据上进一步微调整个模型或部分参数，能最大限度地适应下游任务。</p></li><li><p>预训练+冻结（Feature Extraction）</p><p> 模型预训练后参数不变，只把它当作一个“特征提取器”，在上层加一个简单分类器或回归头。特别适合数据量较小或算力有限的情况</p></li><li><p>参数高效微调（LoRA、Adapter)</p><p> 不微调整个模型，而是在模型中插入一些轻量模块，只训练这些模块带来的少量新增参数</p></li><li><p>零样本&#x2F;少样本学习（Zero-shot&#x2F;Few-shot)</p><p> 直接利用模型的泛化能力，无需额外微调。通过设计提示词（Prompt）进行任务引导，模型不更新参数</p></li></ol><h4 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h4><p>全称Embeddings from Language Models，在论文<a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a>中首次提出，该论文也是NAACL在2018年度的最佳论文。</p><p>ELMO是一种经典的预训练语言模型，具体地说，是一种基于双向LSTM的语言模型。标志着自然语言处理领域从静态词嵌入向上下文相关词表示的重要转折点。与传统的词向量不同，ELMO能根据上下文动态地为同一个词生成不同的向量表示。</p><h5 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h5><p><img src="https://files.catbox.moe/y0l7tk.png" alt="y0l7tk.png"></p><p>核心思想是利用一个双向语言模型BiLM，分别从前向和后向理解文本内容，从而生成上下文相关词向量。</p><p>在传统的前向语言模型中，模型通过给定前文来预测当前词：<br>$$<br>p(w_1,w_2,\dots,w_n)&#x3D;\prod_{k&#x3D;1}^Np(w_k|w_1,w_2,\dots,w_{k-1})<br>$$<br>在后向语言模型中，通过后给定后文来预测当前词：<br>$$<br>p(w_1,w_2,w_n)&#x3D;\prod_{k&#x3D;1}^NP(w_k|w_{k+1},w_{k+2},w_n)<br>$$<br>ELMO就是将二者结合起来，同时追求二者的概率最大化，体现为最大化对数前向和后向的似然概率：<br>$$<br>\begin{aligned}<br>&amp;\sum_{k&#x3D;1}^{N}(:\log p(t_{k}\mid t_{1},\ldots,t_{k-1};\Theta_{x},\overrightarrow{\Theta}<em>{LSTM},\Theta</em>{s})\<br>&amp;+\log p(t_{k}\mid t_{k+1},\ldots,t_{N};\Theta_{x},\overleftarrow{\Theta}<em>{LSTM},\Theta</em>{s}):):<br>\end{aligned}<br>$$</p><h5 id="词向量生成"><a href="#词向量生成" class="headerlink" title="词向量生成"></a>词向量生成</h5><p>对于L层的双向语言模型，每个单词一共有$2*L+1$个表征<br>$$<br>\begin{aligned}<br>R_{k}&amp;&#x3D;\quad{\mathbf{x}<em>{k}^{LM},\overrightarrow{\mathbf{h}}</em>{k,j}^{LM},\overleftarrow{\mathbf{h}}<em>{k,j}^{LM}\mid j&#x3D;1,\ldots,L} \<br>&amp;&#x3D;\quad{\mathbf{h}</em>{k,j}^{LM}\mid j&#x3D;0,\ldots,L}<br>\end{aligned}<br>$$<br>其中，$h_{k,0}^{LM}$为模型输入，对于其它层，$h_{k,j}^{LM}&#x3D;[\overrightarrow{h_{k,j}^{LM}};\overleftrightarrow{h_{k,j}^{LM}}]$</p><p>面对这么多向量，最简单的方法是直接利用顶层$h_{k,L}^{LM}$。更一般地说，针对特定任务计算所有BiLM层的权重：<br>$$<br>\mathbf{ELMo}<em>k^{task}&#x3D;E(R_k;\Theta^{task})&#x3D;\gamma^{task}\sum</em>{j&#x3D;0}^{L}s_{j}^{task}\mathbf{h}_{k,j}^{LM}.<br>$$<br>每层LSTM学到的东西都不一样，针对每个任务每层的重要性也不一样，这就是$\gamma^{task}$和$s_j^{task}$参数的含义</p><h5 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h5><p>首次证明上下文相关词嵌入能显著提升下游任务性能。预训练的ELMo模型可直接作为特征提取器，无需从头训练。但是依然一些局限性：</p><ul><li>基于LSTM，无法实现并行计算。</li><li>无法同时利用上下文，只是将输入分别送给两个模型并最大化总和概率。</li><li>依赖任务特定模式，需为每个任务单独设计模型，在词向量生成公式也可以看出，各任务的权重参数不同。</li></ul><h4 id="发展"><a href="#发展" class="headerlink" title="发展"></a>发展</h4><p>Transformer架构的引入为预训练语言模型提供了新路径。其自注意力机制打破了循环神经网络对序列的依赖，能够并行处理输入序列并动态建模长程依赖关系，大幅提升了模型的表达能力和训练效率。在此基础上，也衍生出了BERT和GPT两种典型模型，分别代表了两种不同的预训练范式。在本篇中主要介绍在自然处理任务上使用较为广泛的BETR模型，其余模型待续…</p><h3 id="BERT-模型"><a href="#BERT-模型" class="headerlink" title="BERT 模型"></a>BERT 模型</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>全称Bidirectional Encoder Representations from Transformers，在论文<a href="http://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a>中首次提出。</p><p>可以作为自然语言处理领域里程碑式的工作，同时还是近年来优秀预训练语言模型的集大成者。BERT的意义在于，从大量无标记数据集中训练得到的深度模型，可以<strong>显著</strong>提高<strong>各项</strong>自然语言处理任务的准确率。</p><ol><li>借鉴ELMO模型的双向编码</li><li>借鉴GPT用trm作为特征提取器的思路</li><li>借鉴word2vec中CBOW方法</li></ol><h4 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a>结构</h4><p>简单的说，模型的结构就是双向连接的多个Encoder组成。从形式上看，BERT之于Transformer就像Word2Vec之于FFNNLM，都是改变了后者的训练目标，剥离出了一部分得到一个专门的预训练模型。</p><p><img src="https://files.catbox.moe/1vha7t.png" alt="1vha7t.png"></p><h4 id="模型训练-1"><a href="#模型训练-1" class="headerlink" title="模型训练"></a>模型训练</h4><p>BERT的训练过程是主要分为两个阶段：预训练和微调。预训练是BERT的核心阶段，基于Trm的Encoder结构，采用<strong>自监督学习</strong>的方式，用语言掩码模型（MLM）方法训练词的语义理解能力；用下句预测（NSP）方法训练句子之间的理解能力，从而更好地支持下游任务。除了输出层之后，微调和预训练使用相同的架构，相同的预训练模型参数用于初始化不同下游任务的模型。在微调过程中，所有参数都会进行微调。</p><p><img src="https://files.catbox.moe/lbl7qi.png" alt="lbl7qi.png"></p><h4 id="语言掩码模型"><a href="#语言掩码模型" class="headerlink" title="语言掩码模型"></a>语言掩码模型</h4><p>借鉴完形填空任务和CBOW的思想，使用MLM方法训练模型。MLM方法简单来说就是随机去掉句子中的部分单词，再让模型来预测被去掉的单词。其中，被去掉的词称为掩码词。在CBOW中，每个词都会作为被预测的中心词，在MLM中则是随机选择15%的词进行掩码。</p><p>考虑到在模型微调训练阶段或模型推理阶段，输入的文本中将没有[MASK]，进而导致产生由训练和预测数据偏差导致的性能损失的的问题，BERT并没有将所有的掩码词替换为[MASK]，而是按照一定比例进行三类替换选项：</p><ul><li>80% 替换为[MASK]</li><li>10% 替换为随机词</li><li>10% 保持不变</li></ul><p>编码器不知道那些词是需要被预测的，哪些词是错误的，被迫需要学习每个token的表示向量。这样也得以让模型通过双向上下文学习词的深层语义表示，捕捉词与词之间的复杂关系。</p><h4 id="下一句预测"><a href="#下一句预测" class="headerlink" title="下一句预测"></a>下一句预测</h4><p>在很多自然语言处理的下游任务中，如问答和自然语言推断，都基于两个句子做逻辑推理，而语言模型并不具备直接捕获句子之间的语义联系的能力，为了<strong>学会捕捉句子之间的语义联系</strong>，BERT 采用了下句预测作为无监督预训练的一部分。</p><p>NSP的训练任务可以描述为，给定两个子句A,B，模型需要判断B是否是A的子句。具体做法是，输入的句子将由两个语句组成，其中，50% 的概率将语义连贯的两个连续句子作为训练文本；另外 50% 的概率将完全随机抽取两个句子作为训练文本。</p><p>输入格式为<code>[CLS]&lt;句子A&gt;[SEP]&lt;句子B&gt;[SEP]</code>，通过训练[CLS]编码后的输出标签，BERT 可以学会捕捉两个输入句对的文本语义。</p><h4 id="输入表示"><a href="#输入表示" class="headerlink" title="输入表示"></a>输入表示</h4><p>为了使BERT能够处理各种下游任务，我们的输入表示法能够在一个标记序列中明确表示一个句子和一对句子。在论文中，句子可以是任意跨度的连续文本，而不是实际的语言句子。序列指的是BERT的输入标记序列，可以是一个句子，也可以是两个句子。</p><p>首先将每个词转化为WordPiece分词，并且第一个标记总是特殊分类标记[CLS]，然后再如下图所示从上到下依次添加词嵌入、片段分割嵌入和位置嵌入。</p><ul><li>Token embed：词嵌入，onre-hot词表映射编码</li><li>Segement embed：分割嵌入，当输入为句子对时，将前一个句子赋值为0，后一个句子赋值为1；只有一个句子则全是0。</li><li>Position embed：位置嵌入，不同与Trm用三角函数表示，是在与预训练过程中训练得到。</li></ul><p><img src="https://files.catbox.moe/e7kvfu.png" alt="e7kvfu.png"></p><h4 id="微调-下游任务改造"><a href="#微调-下游任务改造" class="headerlink" title="微调-下游任务改造"></a>微调-下游任务改造</h4><p><img src="https://files.catbox.moe/guszfr.png" alt="guszfr.png"></p><h5 id="句对分类"><a href="#句对分类" class="headerlink" title="句对分类"></a>句对分类</h5><p>给定两个句子，判断它们的关系，称为句对分类，例如判断句对是否相似、判断后者是否为前者的答案。</p><p>如上图a所示，句对用[SEP]分隔符拼接成文本序列，在句首加入标签[CLS]，将句首标签所对应的输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练。</p><ul><li><p>二分类任务：不需要对输入输出作任何改动，直接利用NSP的训练方法即可。</p></li><li><p>多分类任务：在[CLS]的输出特征向量的后面接上liner+Softmax，保证输出维数和类别数目一致，最后通过argmax操作得到类别结果。</p></li></ul><h5 id="单句分类"><a href="#单句分类" class="headerlink" title="单句分类"></a>单句分类</h5><p>给定一个句子，判断该句子的类别，统称为单句分类，例如判断情感类别、判断是否为语义连贯的句子。</p><p>如上图b所示，单句分类在句首加入标签[CLS]，将句首标签所对应的输出值作为分类标签，计算预测分类标签与真实分类标签的交叉熵，将其作为优化目标，在任务数据上进行微调训练。</p><p>实际上，只有输入与句对分类拜托你，这里不再赘述。</p><h5 id="文本问答"><a href="#文本问答" class="headerlink" title="文本问答"></a>文本问答</h5><p>给定一个问句和蕴含答案的句子，找出答案在后句中的位置，称为文本问答。</p><p>为了标注答案的起始位置和终止位置，引入两个辅助变量$s$和$e$。如上图c所示，将句子B中每一个token得到的特征向量$T_i’$经过全连接层后，分别于向量$s,e$求内积，对所有内积进行Softmax操作，即可得到每个输入词Tok m作为答案起始位置和终止位置的概率，取概率最大的片段即可。</p><p>文本回答任务的微调训练使用了两个技巧：</p><ol><li>用全连接层把 BERT 提取后的深层特征向量转化为用于判断答案位置的特征向量</li><li>引入辅助向量 s 和 e 作为答案其实位置和终止位置的基准向量，明确优化目标的方向和度量方法</li></ol><h5 id="单句标注"><a href="#单句标注" class="headerlink" title="单句标注"></a>单句标注</h5><p>给定一个句子，标注每个次的标签，称为单句标注。</p><p>单句标注任务和 BERT 预训练任务具有较大差异，但与文本问答任务较为相似。如上图d所示，同时是在每个输入词得到的最终语义向量之后添加全连接层，将语义特征转化为序列标注任务所需的特征，单句标注任务需要对每个词都做标注，因此不需要引入辅助向量，直接对经过全连接层后的结果做Softmax操作，即可得到各类标签的概率分布。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SLM </tag>
            
            <tag> NNLM </tag>
            
            <tag> N-gram </tag>
            
            <tag> Transformer </tag>
            
            <tag> PLM </tag>
            
            <tag> ELMo </tag>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>不平衡数据——SMOTE及其改进算法的比较研究</title>
      <link href="/2025/05/02/%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94SMOTE%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83%E7%A0%94%E7%A9%B6/"/>
      <url>/2025/05/02/%E4%B8%8D%E5%B9%B3%E8%A1%A1%E6%95%B0%E6%8D%AE%E2%80%94%E2%80%94SMOTE%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B%E7%AE%97%E6%B3%95%E7%9A%84%E6%AF%94%E8%BE%83%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>不平衡数据集是机器学习特别是分类器模型训练过程中普遍存在的挑战，尤其是在异常检测领域，比如医疗检测、欺诈信息检测等，我们更加关注的是异常类，然而异常数据的发生概率较小，最终导致训练数据中的类别不平衡。当一个类别中的样本数量明显超过另一类时，模型往往会偏向大多数样本，从而导致性能不佳。</p><p>针对数据不平衡问题，可以通过调整损失函数，使模型在少数类出错时承担更高的代价，从而引导模型更多关注少数类。同时，也可以直接从数据源头进行干预，修改数据集以缓解不平衡。具体来说，常用的两种方法是：</p><ol><li>欠采样：通过随机删除或基于某种规则减少多数类样本数量，缺点是删除样本可能会损失有意义的信息</li><li>过采样：通过复制样本或利用样本生成技术增加少数类样本数量，缺点是引入噪声，通过复制而来的样本可能使得模型对某些类别过于敏感</li></ol><p>这里我们主要介绍传统的过采样技术——SMOTE，及其改进算法。</p><h5 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h5><p>在一般情况下，许多分类算法的目标是最大限度的提高分类准确率，而分类准确率是一个偏向多数类的指标。在上文我们提到，异常样本的占比机极小，假定只有0.1%是不合法的，那么什么都不做将所有样本都认为是合法的正确率也有99.9%，但这无疑是没有任何意义的。所以我们需要更适合针对数据不平衡问题的评价指标，<strong>混淆矩阵</strong>是最基本、最直观、计算最简单的方法。</p><p><img src="https://files.catbox.moe/bowh2y.png" alt="bowh2y.png"></p><ul><li>精准率：所有被预测为阳性的样本中真实阳性的占比，强调<strong>预测结果的可靠性</strong>。</li><li>召回率&#x2F;敏感度：所有阳性样本被正确预测为阳性的概率，强调<strong>不能漏掉真正的</strong>。</li><li>特异性：所有阴性样本被正确预测为阴性的概率，强调<strong>不误伤好人</strong>。</li></ul><p>精准率与召回率看上去类似，但实际意义截然不同。当精准率高时，表明预测结果很精准，但可能很多阳性没有被发现；当召回率高时，表明发现了越多的阳性，但可能将部分阴性预测为阳性，导致精准率不那么高。所以，这两者是很难求得两全的标准，为了综合精准率和召回率二者的表现，出现了F1分数，计算公式如下。</p><p>$$<br>\begin{aligned}<br>F1-Score&#x3D;2\times\frac{Precision\times Recall}{Precision+Recall}<br>\end{aligned}<br>$$<br>从公式可以看出，F1 指标对精准率和召回率的权重是相等的，二者并未偏向任何一方。如果需要对精准率和召回率的重要性进行权衡，还可以使用加权的 $F-measure$——F1 的广义形式，$\beta&#x3D;1$时即为F1，$\beta&gt;1$则更重视召回率，反之则更重视精准率。</p><p>$$<br>F-measuer&#x3D;(1+\beta^2)\frac{Precision\times Recall}{(\beta^2\cdot Precision)+Recall}<br>$$<br>上文中提到的精准率、召回率等等指标都是只关注阳性的识别质量，在一些场景如信用卡欺诈检测，我们也不能只关注正类，要同时保证正类和负类都很好才能得到高分，这是G-mean就派上用场。</p><p>$$<br>G-mean&#x3D;\sqrt{Recall\times Specificity}<br>$$</p><h4 id="SMOTE"><a href="#SMOTE" class="headerlink" title="SMOTE"></a>SMOTE</h4><h5 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h5><p><a href="https://www.jair.org/index.php/jair/article/view/10302">论文地址</a></p><p>英文全称 Synthetic Minority Over-Sampling Technique，即<strong>合成少数类过采样技术</strong>，是一种十分经典的过采样技术。正如其名，旨在通过在<strong>特征空间</strong>生成新的少数类样本来弥合差距。简易流程如下图所示。</p><img src="https://www.helloimg.com/i/2025/04/29/6810e83b1ab50.png" alt="6810e83b1ab50.png" style="zoom:150%;" /><h5 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h5><ol><li><p>首先统计各类别的样本数量，识别出少数类样本</p></li><li><p>选定一个少数类样本点，找到它在特征空间内距离最近的K个少数类邻居</p></li><li><p>随机选择一个邻居，在原样本和邻居之间连线，并在该线上进行随机位置插值，生成新的少数类样本<br> $$<br> X_{new}&#x3D;X_i+\lambda\times(X_{ni}-X_i)$$<br> $$<br> 其中$X_i$为选定的样本，$X_{ni}$为随机选择的邻居，$\lambda\in(0,1)$</p></li></ol><h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul><li>虽然通过线性插值的方式避免了直接复制带来的过拟合风险，但是如果少数类分布本身比较分散，线性插值可能生成不合理样本即引入噪声。</li><li>无法解决类别重叠问题，少数类与多数类在特征空间重叠严重，SMOTE可能会进一步加剧混淆。</li><li>对所有少数类样本一视同仁，并未考虑近邻样本的类别信息，往往出现样本混叠现象。</li><li>在存在噪声的情况下，可能会在多数区域生成少数样本，加剧了类内部平衡现象，如下图所示。</li></ul><img src="https://files.catbox.moe/gauk4w.png" alt="gauk4w.png" style="zoom:80%;" /><h4 id="Borderline-SMOTE"><a href="#Borderline-SMOTE" class="headerlink" title="Borderline-SMOTE"></a>Borderline-SMOTE</h4><h5 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h5><p><a href="https://link.springer.com/chapter/10.1007/11538059_91">论文地址</a></p><p>是在SMOTE基础上改进的算法，仅使用边界上的少数类样本合成新样本，从而改善样本的类别分布。更具体地，Borderline-SMOTE作出的改进如下：</p><p>传统smote对所有少数类样本进行过采样；而bl-smoe会先根据邻近样本的类别信息对少数类样本进行如下分类，其中重点关注<strong>危险区域</strong></p><ul><li>Safe安全区域：最近邻中多数类的数量少于一半，认为<strong>少数类聚集且远离多少类</strong>，不必生成样本徒增计算量</li><li>Danger危险区域：最近邻中多少类数量超过一半但不全是，认为<strong>样本靠近边界，容易被错误分类</strong></li><li>Noise噪声区域：最近邻中全部是多数类样本，认为该样本为噪声样本</li></ul><h5 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h5><p>Borderline-SMOTE1：从危险区域的每个样本的k个<strong>少数类</strong>最近邻中随机选择s个，生成合成样本，沿样本与其最近邻之间的线段插值。</p><p>Borderline-SMOTE2添加额外改进：利用最近的多数类邻居进行插值，但$\lambda\in(0,0.5)$，即更靠近少数类，避免类间重叠。</p><p>采样前后对比</p><p><img src="https://files.catbox.moe/d7u0a8.png" alt="d7u0a8.png"></p><h4 id="ADASYN"><a href="#ADASYN" class="headerlink" title="ADASYN"></a>ADASYN</h4><h5 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h5><p><a href="https://ieeexplore.ieee.org/document/4633969?arnumber=4633969">论文地址</a></p><p>全称 adaptive synthetic sampling，即自适应合成抽样，与Borderline-SMOTE相似，同样是基于”SMOTE在合成样本时对所有少数类一视同仁”这一问题加以改进。而<strong>ADASYN</strong>提出，要<strong>重点照顾分类困难的少数类样本</strong>，让模型自适应地更关注这些“难学”的地方。核心思想：</p><ul><li>对于周围多数类较多的少数类，类似bl-smote中的危险区域，被认为是<strong>难学</strong>，生成更多的合成样本</li><li>而对于周围多数类较少的少数类，类似bl-smote中的安全区域，被认为是易学的，生成较少或不生成合成样本</li></ul><h5 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h5><h6 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h6><ul><li>$m_l,m_s$分别为多数类和少数类样本数</li><li>$d_{th}$ 最大容忍阈值</li><li>$\beta$  最终数据集平衡程度</li></ul><h6 id="处理过程"><a href="#处理过程" class="headerlink" title="处理过程"></a>处理过程</h6><ol><li><p>计算不平衡度<br> $$<br> d&#x3D;\frac{m_s}{m_l}<br> $$</p></li><li><p>检查是否需要生成合成样本，$d&lt;d_{th}$ 说明数据集不平衡程度超出了容忍阈值，需要生成合成样本。</p></li><li><p>计算样本数量总数<br> $$<br> G&#x3D;(m_l-m_s)\times\beta<br> $$</p></li><li><p>计算学习难度<br> $$<br> r_i&#x3D;\frac{\Delta_i}K<br> $$<br> 其中 $\Delta_i$ 是K近邻中多数类样本数。$r_i$反映了样本的学习难度，即周围多数类越多，学习难度越大。</p></li><li><p>归一化学习难度，此时$\hat{r_i}$可以看作是概率分布，表示每个少数类样本的<strong>相对权重</strong><br> $$<br> \hat{r}<em>i&#x3D;\frac{r_i}{\sum</em>{i&#x3D;1}^{m_s}r_i}<br> $$</p></li><li><p>根据学习难度分配合成样本数</p></li></ol><p>$$<br>g_i&#x3D;\hat{r_i}\times G<br>$$</p><ol start="7"><li>生成合成样本，在K近邻中随机选择一个少数类样本 $x_{zi}$<br> $$<br> x_{new}&#x3D;x_i+(x_{zi}-x_i)\times\lambda<br> $$</li></ol><p>采样前后对比</p><p><img src="https://files.catbox.moe/kl8nhq.png" alt="kl8nhq.png"></p><h5 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h5><p>从算法流程上可以看出，同样是关注少数类样本周围的样本数量，ADAYSN的适应性更强，并不是硬性区分周围多数类样本超过半数的区域为危险区域，而是利用<strong>学习难度</strong>这一标准加以权衡，让少数类样本之间有着更高的区分度，而不只是简单的危险和安全。</p><h4 id="算法比较"><a href="#算法比较" class="headerlink" title="算法比较"></a>算法比较</h4><p><a href="https://www.diva-portal.org/smash/record.jsf?pid=diva2:1519153&dswid=-1899">论文地址</a> </p><p>该论文对SMOTE和ADASYN算法进行比较分析，主要探究预处理方法、分类器的选择和数据的不平衡程度对模型性能的影响程度。</p><p>过采样设定为使少数类和多数类的样本数尽可能相等，分类模型选择逻辑回归、随机森林、支持向量机。</p><h5 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h5><p>总体上来看，SMOTE和ADAYSN在绝大多数情况都能显著提高分类器性能，表明过采样有效改善了少数类的预测能力。但是并没有一种预处理方法在所有场景下始终优于另一种</p><h6 id="模型的影响"><a href="#模型的影响" class="headerlink" title="模型的影响"></a>模型的影响</h6><ul><li>SVM 随着不平衡程度的增加，SMOTE+SVM的组合表现更佳且均优于ADAYSN</li><li>RF  ADAYSN表现较优，随着不平衡程度增加，性能提升更明显 </li><li>LR  在不平衡程度最高的信用卡欺诈数据中，过采样反而降低了性能，可能是由于合成样本引入了噪声</li></ul><h5 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h5><p>过采样率统一设为完全平衡（50:50），可能不适用于所有数据集，论文建议在某些场景下尝试较低的过采样率或使用其他方法以减少合成样本的负面影响。</p><p>实际上，更加值得考虑的问题是，Borderline-SMOTE和ADAYSN都只是针对“传统SMOTE合成样本时对所有少数类一视同仁“这一问题进行改进，并没有解决生成不合理样本和类别重叠问题，单纯的过采样反而加重<strong>边界模糊</strong>，特别是在少数类样本分散或存在离群点时。</p><h4 id="K-means-SMOTE"><a href="#K-means-SMOTE" class="headerlink" title="K-means SMOTE"></a>K-means SMOTE</h4><h5 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h5><p><a href="https://linkinghub.elsevier.com/retrieve/pii/S0020025518304997">论文地址</a></p><p>引言部分指出，在类不平衡的情况下，改进分类技术可以分为三大类：算法级方法、数据级方法、成本敏感方法。算法级方法受限于分类器，相比之下，对成本敏感的方法旨在为每个类别提供不同误分类成本的分类算法。这就需要了解误分类成本，而误分类成本取决于数据集，通常是未知的或难以量化的。所以，数据级方法因其通用性得以普遍适用。</p><p>数据集方法可以进一步划分为随机方法和知情方法。随机方法是随机选择要删除和复制的样本，而知情方法则不同。它考虑到了样本的分布情况。这样，有信息的方法就能将精力集中在输入空间的关键区域，例如Borderline-SMOTE中的安全区域。因此，有信息的方法可以避免产生噪音，并能解决类内的不平衡问题。</p><h5 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h5><p>结合K-means聚类和SMOTE的过采样方法，旨在消除类间不平衡和类内不平衡问题。</p><blockquote><p>k-means是一种非常常用的无监督聚类算法，目标将数据划分为K个簇，使得每个簇内的数据点尽可能相似，而不同簇之间的数据点尽可能不同。它的核心思想是<strong>最小化簇内样本的平方误差</strong>。</p><p>算法流程:</p><ol><li>初始化：随机初始化聚类中心。</li><li>分配样本：每个样本分配到最近的聚类中心，形成K个簇。</li><li>更新中心：重新计算各簇所有点的均值，作为新的聚类中心。</li><li>重度23操作，直到聚类中心不再变化或达到最大迭代次数。</li></ol></blockquote><h5 id="算法流程-1"><a href="#算法流程-1" class="headerlink" title="算法流程"></a>算法流程</h5><h6 id="聚类预处理"><a href="#聚类预处理" class="headerlink" title="聚类预处理"></a>聚类预处理</h6><p>使用k-means聚类算法将整个数据集划分为k个簇，聚类的目的是识别数据的自然分组，为后续的过采样提供结构化的基础。该步骤的核心目标是<strong>识别生成合成样本区域</strong>。</p><h6 id="过滤与采样分配"><a href="#过滤与采样分配" class="headerlink" title="过滤与采样分配"></a>过滤与采样分配</h6><ol><li><p>根据每个簇的不平衡比率选择适合采样的簇，不平衡程度的定义如下<br> $$<br> IR(c)&#x3D;\frac{\text{majority count}(c)+1}{\text{minority count}(c)+1}<br> $$<br> 默认情况下，选择少数类样本占比至少为50%的簇（$IR(c)&lt;1$），也可以通过$irt$来控制</p></li><li><p>计算每个簇的稀疏度，作为采样权重，稀疏的少数类簇获得更高的采样权重。下面分别是密度、稀疏度、采样权重,采样数量的计算公式<br> $$<br> \begin{aligned}<br> &amp;\text{density}(f)&#x3D;\frac{\text{minority count}(f)}{\text{average minority distance}(f)^{de}} \<br> &amp;\text{sparsity}(f)&#x3D;\frac1{\text{density}(f)} \<br> &amp;\text{sampling weight}(f)&#x3D;\frac{\text{sparsity}(f)}{\sum\text{sparsity}(f)} \<br> &amp;\text{number of samples}&#x3D;\left \lfloor n \times \text{sampling weight}(f) \right \rfloor<br> \end{aligned}<br> $$</p></li></ol><h6 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h6><p>依然采用样本和邻居之间进行线性插值的方法，随机选择簇内<strong>少数类</strong>样本$\vec{a}$，在其k近邻中随机选择<strong>少数类</strong>样本$\vec{b}$，生成新样本$\vec{x}$，$w\in[0,1]$是随机权重</p><p>$$\vec{x}&#x3D;\vec{a}+w\times(\vec{b}-\vec{a})$$</p><h5 id="评价-1"><a href="#评价-1" class="headerlink" title="评价"></a>评价</h5><p>观察算法流程，在过滤操作和Borderline-line比较类似，都是对不同少数类样本加以区分或者关注；在生成样本数量分配上，和ADAYSN算法过程比较类似，增加对稀疏区域的采样数量。该算法得以效果显著的原因在于 K-means预处理。</p><p>K-means是一种<strong>无监督</strong>的聚类方法，不依赖于类标签，直接对整个输入空间进行聚类。这允许算法发现<strong>类重叠区域</strong>，并通过过滤步骤排除这些区域，仅在少数类主导的簇内过采样。如下图所示，最终得到过采样数据校正决策边界。</p><p><img src="https://www.helloimg.com/i/2025/04/29/6810e299cc890.png" alt="6810e299cc890.png"></p><p>SMOTE和k-means SMOTE过采样之后的数据分布对比，可以明显看出k-means SMOTE避免在重叠区域生成样本，保护了决策边界的清晰性，改善分类器的泛化能力。</p><p><img src="https://www.helloimg.com/i/2025/04/29/6810d65055836.png" alt="6810d65055836.png"></p><p>与其它聚类算法的比较</p><ul><li>CURE-SMOTE：使用层次聚类清除噪声，但其复杂度较高，未解决类内不平衡问题</li><li>SOMO：将数据映射到二维空间，增加了预处理的复杂性，还可能丢失掉高维信息</li><li>Cluster-SMOTE：仅聚类少数类样本，无法识别类重叠区域，K-means的全局聚类更加全面</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 数据不平衡 </tag>
            
            <tag> 过采样 </tag>
            
            <tag> SMOTE </tag>
            
            <tag> Borderline-SMOTE </tag>
            
            <tag> ADAYSN </tag>
            
            <tag> SMOTE改进 </tag>
            
            <tag> KMeansSMOTE </tag>
            
            <tag> K-means </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>马尔可夫链</title>
      <link href="/2025/04/28/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/"/>
      <url>/2025/04/28/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/</url>
      
        <content type="html"><![CDATA[<h3 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h3><h4 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h4><p>概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（天气随着时间的变化）。在随机过程中，随机现象在某时刻$t$的取值用$S_t$表示，所有可能的状态组成状态集合$S$。</p><h4 id="马尔可夫性质"><a href="#马尔可夫性质" class="headerlink" title="马尔可夫性质"></a>马尔可夫性质</h4><p>当且仅当某时刻的状态只取决于上一个时刻的状态时，一个随机过程被称为具有<strong>马尔可夫性质</strong>。更系统地，当前状态是未来的充分统计量，而不会受到过去状态的影响。但是这并不代表和历史完全没有关系，实际上当前状态也受到了上一状态的影响，正是通过这种链式关系将历史信息的影响传递到了现在。显然，这一性质可以大大简化计算。</p><h4 id="马尔可夫过程"><a href="#马尔可夫过程" class="headerlink" title="马尔可夫过程"></a>马尔可夫过程</h4><p>又叫马尔可夫链，即满足马尔可夫性质的随机过程，通常用一个<strong>有限状态集合</strong>和一个<strong>状态转移矩阵</strong>来表示。直观地，马尔可夫链可以看作一个有向图，其中每条边的权值表示从发出状态到接受状态之间的转移概率，所以状态转移矩阵其实可以说是邻接矩阵。</p><h4 id="静态分布"><a href="#静态分布" class="headerlink" title="静态分布"></a>静态分布</h4><p>马尔可夫链在长时间运行后达到的一种稳定状态的概率分布。直观地，假设静态分布为 $\pi$，意味着经过足够长的时间后，系统状态的分布不再变化且每个状态$s_i$出现的概率为 $\pi_i$</p><h5 id="为什么？"><a href="#为什么？" class="headerlink" title="为什么？"></a>为什么？</h5><p>假设状态转移矩阵为 $A$， $P_{ij}(n)$ 表示状态$i$到$j$且恰好经过$n$步的概率 </p><p>$$\begin{aligned}P_{i}j(n)&amp;&#x3D;\sum_kP_{ik}(r)\times P_{kj}(n-r)\&amp;&#x3D;A_{ij}^n\end{aligned}$$</p><p>随着步数的增加，初始状态的影响越来越小，最终整个系统的行为只由转移该来本身决定。所以转移矩阵在经过多次幂运算后逐渐趋于稳定，如下式子所示。每一行都是正态分布$\pi$，所以无论起点如何（处于矩阵的那一行），在经过足够长的时间后（$n\to\infty$)，落到所有状态的概率都是固定的（每一列的取值都相等）。</p><p>$$\lim_{n\to\infty}A^n&#x3D;\begin{bmatrix}\pi\\pi\\vdots\\pi\end{bmatrix}$$</p><p>当然，静态分布并不是在所有情况下都成立，需要满足以下条件</p><ul><li>可约性，所有状态必须是互相到达的</li><li>无周期性，不会被卡在循环内</li></ul><p>也就是说，满足静态分布性质的马尔可夫链所构成的图是一个<strong>无环的强连通图</strong>。</p><h3 id="MM用于文本生成任务"><a href="#MM用于文本生成任务" class="headerlink" title="MM用于文本生成任务"></a>MM用于文本生成任务</h3><p>上面提到的<strong>状态</strong>可以表示为文本中的词，根据马尔可夫性质，仅通过当前词来预测下一个词。</p><p>一种比较简单的方法为，在大量样本数据上学习到各个词语之间的转移概率，从给定起始词出发进行随机漫步从而达到生成文本的效果。虽然生成的文本大多数是无意义的，但是大多数都是语法正确的。因为语法成分具有很强的前后依赖关系，比如形容词后大多是名词，名词后大多是动词等。</p><h3 id="HMM-隐马尔可夫模型"><a href="#HMM-隐马尔可夫模型" class="headerlink" title="HMM_隐马尔可夫模型"></a>HMM_隐马尔可夫模型</h3><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>HMM 是一种可用于标注任务的机器学习模型，描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型。</p><p>HMM 包含 隐藏的即不可观测的序列 和 可观测序列，隐藏序列具有 马尔可夫性质，即当前状态仅有上一状态决定。其中，可观测序列仅由当前的隐藏状态决定，模型的目标是&#x3D;&#x3D;通过可观测序列找到最有可能的隐藏状态序列&#x3D;&#x3D;。实际上，刚才的的描述已经了包含了 HMM 的两个关键假设，即 <strong>马尔可夫假设和观测独立性假设</strong>。</p><h4 id="模型描述"><a href="#模型描述" class="headerlink" title="模型描述"></a>模型描述</h4><p>包含隐藏状态集合，观测状态集合，初始概率，转移概率和发射概率。初始概率即为稳态分布，发射概率即为从隐藏状态到观测状态的概率。</p><h4 id="序列标注"><a href="#序列标注" class="headerlink" title="序列标注"></a>序列标注</h4><h5 id="解释性"><a href="#解释性" class="headerlink" title="解释性"></a>解释性</h5><p>在标注任务中，<strong>可观测序列就是文本，隐藏状态序列就是所求的标签</strong>。我们有充分理由认为，当前词的词性或者槽仅有前一个词的词性或者槽标签决定，也就是说&#x3D;&#x3D;马尔可夫假设是有说服力的&#x3D;&#x3D;。在语料库中，我们也可以学习到各词性或者槽标签出现最多的词是哪些，所以发射概率是可求的。</p><ul><li>转移概率：描述了”上一个标签是什么，接下来可能是什么标签”</li><li>发射概率：描述了“这个标签下，出现这个观测值的可能性”</li></ul><h5 id="具体应用"><a href="#具体应用" class="headerlink" title="具体应用"></a>具体应用</h5><ul><li><p>首先，从语料库中学习到不同词性或者槽标签的之间的转移概率，再通过这个概率求出稳态分布</p></li><li><p>其次，统计各个词性或者槽标签下出现某词的概率即发射概率</p></li></ul><p>有了这三个概率后，就可以根据给定的 观测值即输入序列 得到 隐藏状态序列即标签序列，假设$X$为可观测序列，$Y$为隐藏状态序列，对问题进行概率建模，再利用贝叶斯定理进行展开。</p><p>$$<br>\begin{aligned}<br>&amp;argmax_{X&#x3D;X_1,X_2,\ldots,X_n}P(X&#x3D;X_1,X_2,\ldots,X_n\mid Y&#x3D;Y_1,Y_2,\ldots,Y_n)\<br>&amp;\to argmax_{X&#x3D;X_1,X_2,\dots,X_n}\frac{P(Y|X)P(X)}{P(Y)} \<br>&amp;P(Y|X)&#x3D;P(Y_1|X_1)<em>P(Y_2|X_2)</em>\dots P(Y_n|X_n)&#x3D;\prod P(Y_i|X_i)\<br>&amp;P(X)&#x3D;\prod P(X_i|X_i-1)\<br>&amp;argmax_{X&#x3D;X_1,X_2,…X_n}\prod P(Y_i\mid X_i):P(X_i\mid X_{i-1})<br>\end{aligned}<br>$$</p><h5 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h5><p>本质是一种&#x3D;&#x3D;动态规划&#x3D;&#x3D;算法</p><p>参数说明：</p><ul><li><p>$\pi(i)$ 为$S_i$初始概率</p></li><li><p>$P_1(i,j)$ $S_i$到$S_j$的转移概率</p></li><li><p>$P_2(i,O_t)$ 表示$S_i$生成观测值$O_t$的发射概率</p></li></ul><p>状态定义：</p><ul><li>$\delta_t(j)$ 时间$t$时到达$S_j$的最大概率</li><li>$\psi_t(j)$ 记录上一时刻的状态，用于回溯找出路径</li></ul><p>状态转移矩阵：</p><p>$$\begin{aligned}\delta_t(j)&#x3D;\max_i\left(\delta_{t-1}(i)\times P_1(i,j)\right)\times P_2(j,O_t)\\psi_t(j)&#x3D;\arg\max_i\left(\delta_{t-1}(i)\times P_1(i,j)\right)\end{aligned}$$</p><p>路径回溯：</p><p>$$\begin{aligned}q_T^<em>&#x3D;\arg\max_j\delta_T(j)\q_{t-1}^</em>&#x3D;\psi_t(q_t^*)\quad(t&#x3D;T,T-1,\ldots,2)\end{aligned}$$</p><h3 id="MEMM-最大熵马尔可夫模型"><a href="#MEMM-最大熵马尔可夫模型" class="headerlink" title="MEMM_最大熵马尔可夫模型"></a>MEMM_最大熵马尔可夫模型</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><h5 id="HMM-的缺陷"><a href="#HMM-的缺陷" class="headerlink" title="HMM 的缺陷"></a>HMM 的缺陷</h5><p>在 HMM 中提到，HMM 解决的任务的需要满足 马尔可夫假设 和 观测独立性建设。在序列标注任务中，马尔可夫假设是很有说服力的，即当前标签仅跟标签仅跟上一个标签有关，不需要更早的标签。</p><p>但是观测独立性假设意味着，观测值只依赖于当前隐藏状态，虽然有一定的可解释性，但过于简单并不能自由地利用上下文信息。在实际情况中，往往需要更多的特征。HMM并不能很好的利用丰富特征</p><h4 id="最大熵原理"><a href="#最大熵原理" class="headerlink" title="最大熵原理"></a>最大熵原理</h4><p>在已知部分信息的情况下，对未知概率分布进行推断或选择的原则。核心思想是，在所有满足已知约束条件的概率分布中，选择那个<strong>熵最大</strong>的分布作为最佳或最合理的推断。</p><p>应用：对于多种复杂特征，我们并不知道哪些特征能够更好的辅助预测当前词性，所以做出最无偏的估计，最大化不确定性，保留最大的可能性空间，最大程度上避免引入不必要的假设。</p><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><p>灵活加入任意特征，通过最大化不确定性来避免主观假设，还降低过拟合风险。这是 MEMM 相对于 HMM 的巨大优势。它可以把当前观测的各种特征（比如词本身、词根、词缀、大小写、是不是数字、前后词是什么等） 都作为线索，帮助预测当前词性。HMM 则很难利用这么多交叉重叠的特征。</p><h5 id="标注偏见"><a href="#标注偏见" class="headerlink" title="标注偏见"></a>标注偏见</h5><p>局部最优解不一定走向全局最优解。虽然相比HMM添加了很多特征，但决策依然是局部的，在上一个状态的基础上，结合特征向前推进，从而忽略了全局最优的决策。从HMM的求解算法中我们提到，维特比算法本质是一种动态规划算法，而动态规划能够求解最优解的前提是<strong>问题最优解可以由子问题的最优解推导出来</strong>，在复杂的序列标注问题中，显然不满足这一特点。</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HMM </tag>
            
            <tag> MEMM </tag>
            
            <tag> 序列标注 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本数据处理方法</title>
      <link href="/2025/04/27/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"/>
      <url>/2025/04/27/%E6%96%87%E6%9C%AC%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="什么是特征工程？"><a href="#什么是特征工程？" class="headerlink" title="什么是特征工程？"></a>什么是特征工程？</h3><p>简单来说是对数据进行处理，为模型提供有效的输入表示</p><p>目的：将文本、图像等原始数据转成数字化、结构化的形式供模型使用，在 nlp 任务则主要针对文本数据</p><h3 id="传统基于统计的方法"><a href="#传统基于统计的方法" class="headerlink" title="传统基于统计的方法"></a>传统基于统计的方法</h3><h4 id="BOW"><a href="#BOW" class="headerlink" title="BOW"></a>BOW</h4><p>非常经典且简单的文本特征表示方法<br>基本思想：</p><ol><li>忽略词语的顺序和语法结构，只关注词语的频率特征</li><li>将文本转为固定长度的向量</li></ol><p>步骤：</p><ol><li>统计并去重训练文本中所有单词，构成词汇表</li><li>针对单一文本，统计每个词的出现次数形成文本特征向量</li></ol><p>最终形成的向量长度等于词汇表的大小，对应位置表示对应单词的出现次数</p><h5 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h5><p>优点：简单高效易实现</p><p>缺点：忽略单词顺序和语法结构、无法处理训练数据中未出现的词，最终向量高维稀疏</p><h4 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h4><p>实际上是一种<strong>加权技术</strong>， 评估一字词对于整个语料库的重要程度 </p><p>基本思想是：如果只在某一篇文章中词频高，而在其余文章中很少出现，则可以认为其具有很好的类区分能力，适合用来分类</p><p>两个指标 TF是词频(Term Frequenc)，IDF是逆向文件频率(Inverse Document Frequency)</p><p>$$\begin{aligned}&amp;TF(t,d)&#x3D;\frac{f(t,d)}{N(d)}\&amp;IDF(t)&#x3D;log(\frac N{D_t+1})\end{aligned}$$</p><p>通过计算公式可以看出，TF表示词语在当前文档的重要性，而 IDF 衡量了词语在整个文档&#x2F;语料库的重要性<br>$$TF-IDF_{t,d}&#x3D;TF(t,d)\times IDF(d)$$</p><p>主要应用：搜索引擎、关键词提取、文本相似性、文本摘要</p><h3 id="基于推理-预测的方法"><a href="#基于推理-预测的方法" class="headerlink" title="基于推理&#x2F;预测的方法"></a>基于推理&#x2F;预测的方法</h3><h4 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h4><p>定义：将单词映射到低维连续向量空间的方式，通过训练模型来获得每个单词的向量表示</p><p>作用：将原本稀疏且高维的矩阵转变为低维且连续的矩阵，一般为100~300维</p><p>特点：词向量内积或余弦表示单词间的相似性。也就是说如果两个词的语义很相近，那么它们的空间位置越近</p><p>注意：在实际运用中可以选择使用在大型语料库中预训练的词向量，但这对于一些垂直领域的任务可能不太适用，因为预训练的词向量可能无法很好地捕捉特定领域的语义信息</p><img src="https://files.catbox.moe/q4yll6.png" alt="q4yll6.png|610" style="zoom: 67%;" /><h5 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h5><p>根据向量空间位置分布的特点，词向量或者说单词的分布式表示可以用来查找近似单词，但其使用场景还远不止此，其中一个非常重要的作用就是<strong>迁移学习</strong>——指在某个领域学习到的知识可以被应用于其它领域。</p><p>在上述提供的传统的基于统计的方法，更多的关注词频等信息，对任务的耦合程度极高，可以说脱离了原有训练数据就毫无意义。但是，在几乎所有类型的自然语言处理任务（分类、聚类、标注和情感分析等），单词的分布式表示都有很好的效果。</p><p>正因如此，虽然后面会花费大篇幅介绍词向量的训练过程，但是大多数时候我们可以直接使用在大型语料库中学习好的词向量。</p><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>一种学习词向量的方法<br>有以下两种训练方法，利用神经网络，通过最大化预测内容的正确性来<strong>学习</strong>到词向量表示。</p><p><img src="https://www.helloimg.com/i/2025/04/27/680e123707cb8.png" alt="680e123707cb8.png"></p><h5 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h5><p>连续词袋模型，通过上下文来预测中间词，下图可以简单描述这一过程。在自然语言处理领域，理解单词的含义通常需要考虑上下文，因为上下文提供单词用法和语义的重要线索。</p><h6 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h6><p>为了简单起见，我们先只考虑上下文窗口为1的情况，即只取前后1个单词</p><p>概率建模：$P(w_t|w_{t-1},w_{t+1})$ 表示 “给定上下文$w_{t-1},w_{t+1}$时目标词为$w_t$的概率”</p><p>套用<strong>交叉熵误差函数</strong>，得到如下公式</p><p>$$L&#x3D;-\frac1T\sum_{t&#x3D;1}^T\log P(w_t|w_{t-1},w_{t+1})$$</p><p>学习的任务就是让上式表示的损失函数尽可能小</p><h5 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h5><p>跳元模型，与CBOW 完全相反，通过单词来预测上下文 </p><h6 id="损失函数-1"><a href="#损失函数-1" class="headerlink" title="损失函数"></a>损失函数</h6><p>为了简单起见，我们依然只考虑上下文窗口为1的情况，即预测结果为前后1个词</p><p>由于输出层不止一个，因此损失函数要计算各输出层的总和</p><p>概率建模：$P(w_{t-1},w_{t+1}|w_t)$ 表示 “给定$w_t$时，$w_{t-1},w_{t+1}$同时出现的概率”</p><p>同样使用交叉熵误差函数，如果只考虑上述情况</p><p>$$\begin{aligned}L&amp;&#x3D;-\log P(w_{t-1},w_{t+1}|w_{t})\&amp;&#x3D;-\log P(w_{t-1}|w_t)P(w_{t+1}|w_t)\&amp;&#x3D;-(\log P(w_{t-1}|w_t)+\log P(w_{t+1}|w_t))\end{aligned}$$</p><p>推广到整个语料库</p><p>$$L&#x3D;-\frac1T\sum_{t&#x3D;1}^T(\log P(w_{t-1}|w_t)+\log P(w_{t+1}|w_t))$$</p><h5 id="两种方法的比较"><a href="#两种方法的比较" class="headerlink" title="两种方法的比较"></a>两种方法的比较</h5><p>从CBOW和skip-gram预测过程上看：CBOW通过中间词对周围词进行调整，预测次数跟整个文本的次数基本是相等。而在skip-gram中，每个词作为输入时，都会受到周围词的影响，相当于比CBOW多进行了K次，K为上下文窗口大小。</p><p>所以，skip-gram训练出的词向量往往会有更好的表现，但是计算成本较高。</p><h4 id="Word2Vec-高速化"><a href="#Word2Vec-高速化" class="headerlink" title="Word2Vec 高速化"></a>Word2Vec 高速化</h4><p>word2vec的实现原理和训练过程都比较简单。但是在实际的使用场景中，词表往往有成千上万的单词，词汇量增大所带来的的计算效率问题是当前我们研究的问题。</p><p>实际上，效率问题主要是sfotmax层计算带来的，观察softmax计算公式，分母要进行V次exp计算</p><p>$Softmax(z_i)&#x3D;\frac{exp(z_i)}{\sum_j^Vexp(z_j)}$</p><p>因此我们需要可以替代softmax的<strong>轻量计算</strong></p><h5 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h5><p>最初被用于加速skip-gram模型训练，后来被广泛用于自然语言处理，计算机视觉和推荐系统中。</p><h6 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h6><p>在训练时，不是对所有可能的样本进行训练，而是只针对<strong>正样本和少量随机选出来的负样本</strong>来训练</p><ul><li>正样本：真实的上下文词，模型希望预测<strong>正确</strong>的配对</li><li>负样本：随机的无关词，模型希望预测<strong>错误</strong>的配对</li></ul><p>这样一来，模型训练的目的不只是最大化正确的概率，还要最小化负样本正确的概率。</p><p>从任务上来看，我们用&#x3D;&#x3D;二分类来拟合多分类效果，从”是什么”到”是什么吗”的转变&#x3D;&#x3D;</p><p>带来两个主要好处</p><ul><li>大大减少训练的计算量</li><li>同时关注正确的搭配和错误的搭配</li></ul><h6 id="损失函数-2"><a href="#损失函数-2" class="headerlink" title="损失函数"></a>损失函数</h6><p>在多分类问题中，通常使用softmax将得分转化为概率。而对于二分类问题，并不需要归一化操作，输出层仅使用简单的sigmod函数即可</p><p>$$\sigma&#x3D;\frac{1}{1+e^{-x}}$$</p><p>$$h&#x3D;\frac{1}{m}\sum_{i&#x3D;1}^mv_{c_i}$$</p><p>$$L&#x3D;-\left(\log\sigma(v_w^\prime\cdot h)+\sum_{k&#x3D;1}^K\log\sigma(-v_{w_k}^\prime\cdot h)\right)$$</p><p>其中$v$代表学习到的词向量， $h$ 为上下文综合向量，其中前半部分为正采样，后半部分为负采样，$K$为负采样个数</p><h5 id="层次-softmax"><a href="#层次-softmax" class="headerlink" title="层次 softmax"></a>层次 softmax</h5><h6 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h6><p>将词表组织成一棵二叉树</p><ul><li>词表中的每个词放置在叶子节点</li><li>每个中间节点都有自己的词向量</li><li>每经过一个节点，都在做一次二分类</li></ul><p>类似于在二叉搜索树上找到指定元素的过程，将复杂度从$O(N)$降低到$O(logN)$</p><h6 id="损失函数-3"><a href="#损失函数-3" class="headerlink" title="损失函数"></a>损失函数</h6><p>每个节点向下做出决策时依然是二分类，依然使用sigmod函数来表示概率</p><p>$$P(w)&#x3D;\prod_{i\in path(w)}P(i)$$ </p><p>$$L&#x3D;-\sum_{i\in path(i)}logP(i)$$</p><p>$path(w)$为跟节点到单词为w的叶子节点的路径上的节点集合，$P(i)$为概率</p><p>训练的目标是最大化正确路径上所有节点决策的概率</p><h4 id="Glove"><a href="#Glove" class="headerlink" title="Glove"></a>Glove</h4><p>遍历整个语料库来构建共现矩阵，优化目标函数<br>Word2Vec 更关注局部上下文信息，而Glove 语义捕捉更全面</p><h5 id="共现矩阵"><a href="#共现矩阵" class="headerlink" title="共现矩阵"></a>共现矩阵</h5><p>在word2vec中，通过滑动窗口选取输入样本</p><p>在Glove中，直接对窗口内的每对词进行词频统计，遍历整个语料库，从而得到共现矩阵</p><h5 id="平滑损失函数"><a href="#平滑损失函数" class="headerlink" title="平滑损失函数"></a>平滑损失函数</h5><p>$$\begin{aligned}<br>J&#x3D;\sum_{i,j&#x3D;1}^Vf(X_{ij})(w_i^Tw_j+b_i+b_j-\log(X_{ij})^2) \<br>f(x)&#x3D;(\frac{x}{x_{max}})^{\alpha} \end{aligned}$$</p><p>其中，$X_{i,j}$ 为特定大小窗口的共现次数，$W_{i,j}$ 表示学习到的词向量</p><p>$f$ 为加权函数，用来调整每个词对损失的贡献，减对于高频词语的过分关注，更好的处理停用词等</p><h3 id="后续发展"><a href="#后续发展" class="headerlink" title="后续发展"></a>后续发展</h3><p>随着文本表示方法的不断演进，我们对语言的建模方式也在不断提升。从早期基于词频、共现统计的方法，到如今依赖上下文语义、深度学习的技术路径，文本处理与语言建模之间的界限越来越模糊。特别是在引入神经网络和预训练模型之后，文本处理不再只是简单的特征抽取，而是成为了语言理解与生成的基础能力。</p><p>可以参考我另一篇博文：<a href="https://blog.bayh.top/2025/05/04/%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF%E6%BC%94%E8%BF%9B%E7%BB%BC%E8%BF%B0/">语言模型的技术演进综陈述：从N-gram到BERT</a>，其中详细梳理了语言模型的技术路线与核心思想的转变过程。</p>]]></content>
      
      
      <categories>
          
          <category> 自然语言处理 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> word2vec </tag>
            
            <tag> TF-IDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>莫比乌斯反演</title>
      <link href="/2025/04/26/%E8%8E%AB%E6%AF%94%E4%B9%8C%E6%96%AF%E5%8F%8D%E6%BC%94/"/>
      <url>/2025/04/26/%E8%8E%AB%E6%AF%94%E4%B9%8C%E6%96%AF%E5%8F%8D%E6%BC%94/</url>
      
        <content type="html"><![CDATA[<h2 id="莫比乌斯反演"><a href="#莫比乌斯反演" class="headerlink" title="莫比乌斯反演"></a>莫比乌斯反演</h2><h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>反演实际上是利用莫比乌斯函数实现 &#x3D;&#x3D;和函数与原函数之间的转换&#x3D;&#x3D;</p><p>我们需要求原函数，而实际上和函数是容易求得的</p><h4 id="常用反演规律"><a href="#常用反演规律" class="headerlink" title="常用反演规律"></a>常用反演规律</h4><p>$f(n)$ 表示..为n的方案数</p><p>$F(n)$ 表示..为n的倍数的方案数</p><p>$F(n)&#x3D;\sum_{n|d}f(d)\\text{反演}\to f(n)&#x3D;\sum_{n|d}F(d)\mu(\frac{d}{n})$</p><h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p><a href="https://www.luogu.com.cn/problem/P1390">公约数的和 可以当作模版题</a></p><p>$\sum_{i&#x3D;1}^n\sum_{j&#x3D;i+1}^ngcd(i,j)\f(n)&#x3D;\sum\sum [gcd(i,j)&#x3D;n]\F(n)&#x3D;\sum_{n|d}f(d)\\text{反演}\to f(n)&#x3D;\sum_{n|d}F(d)*\mu(\frac{d}{n})\Ans&#x3D;\sum_{i&#x3D;1}^nf(i)*i$</p><p>这一题还可以对式子进行变换，对于j&#x3D;i+1这种和式超级难受</p><p>$\sum_{i&#x3D;1}^n\sum_{j&#x3D;i+1}^ngcd(i,j)\times2+\sum_{i&#x3D;1}^ni&#x3D;\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^ngcd(i,j)$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// de(&quot;\n\n&quot;);</span></span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">getMu</span>();</span><br><span class="line">    <span class="type">int</span> ans=<span class="number">0</span>;</span><br><span class="line">    <span class="comment">//先算出F[i]的值，公倍数为i的倍数的方案数</span></span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        F[i]=<span class="number">0</span>;</span><br><span class="line">        <span class="comment">//枚举倍数，公倍数为j 乘法原理计算方案</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=i;j&lt;=n;j+=i)&#123;</span><br><span class="line">            F[i]+=(n/i-j/i);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        f[i]=<span class="number">0</span>;</span><br><span class="line">        <span class="comment">//枚举倍数d 根据反演式计算F[i]</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> d=i;d&lt;=n;d+=i)&#123;</span><br><span class="line">            f[i]+=F[d]*mu[d/i];</span><br><span class="line">        &#125;</span><br><span class="line">        ans+=f[i]*i;</span><br><span class="line">    &#125;</span><br><span class="line">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>式子变换之后的写法</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// de(&quot;\n\n&quot;);</span></span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">init</span>();</span><br><span class="line">    <span class="type">int</span> ans=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,n)  F[i]=(n/i)*(n/i);</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        f[i]=<span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> d=i;d&lt;=n;d+=i)&#123;</span><br><span class="line">            f[i]+=F[d]*mu[d/i];</span><br><span class="line">        &#125;</span><br><span class="line">        ans+=f[i]*i;</span><br><span class="line">    &#125;</span><br><span class="line">    ans-=(<span class="number">1</span>+n)*n/<span class="number">2</span>;</span><br><span class="line">    ans/=<span class="number">2</span>;</span><br><span class="line">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://www.luogu.com.cn/problem/P1447">能量采集</a></p><p><a href="https://ac.nowcoder.com/acm/contest/22769/C">序列</a></p><p>$\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^n[gcd(i,j)&#x3D;1]$ </p><p>f(n)表示gcd为n的方案数，F(n)表示gcd为n的倍数的方案数，所求答案即为f(1)</p><p><a href="https://www.luogu.com.cn/problem/P2231">跳蚤</a></p><p>$\sum_{a&#x3D;1}^m\sum_{b}^m\dots[gcd(a,b,\dots)&#x3D;1]$</p><p>相同的方法反演，使用乘法原理就可以算出gcd为k的倍数的方案数$\to(\frac{m}{d})^n$</p><p><a href="https://ac.nowcoder.com/acm/contest/22769/E">公约数之和加强版</a></p><p>$\sum_{a&#x3D;1}^k\sum_{b}^k\dots.gcd(a,b,\dots)$</p><p><a href="https://ac.nowcoder.com/acm/contest/22769/D">LCMS</a></p><p>和式可以变换为 $\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^nlcm(A_i,A_j)$</p><p>$f(n)&#x3D;\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^n(A_i\times A_j)[gcd(i,j)&#x3D;&#x3D;n]\F(d’)&#x3D;\sum_{d’|d}f(d)&#x3D;\sum_{d’|d}\sum_{i&#x3D;1}^n[\frac{A_i}{d}]\sum_{j&#x3D;1}^n[\frac{A_j}{d}]$$</p><p>上面的式子是比较好算的，交叉相乘为和的乘积，利用反演将原函数用上述和函数表示</p><p>$f(n)&#x3D;\sum_{n|d}F(d)\mu(\frac{d}{n})\ans&#x3D;\sum_{d&#x3D;1}^n\frac{1}{d}f(d)$</p><p><a href="https://www.luogu.com.cn/problem/P3911">最小公倍数之和</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// de(&quot;\n\n&quot;);</span></span><br><span class="line">    cin&gt;&gt;n;</span><br><span class="line">    <span class="built_in">init</span>(N<span class="number">-10</span>);</span><br><span class="line">    <span class="type">int</span> sum=<span class="number">0</span>,mx=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        cin&gt;&gt;a[i];</span><br><span class="line">        cnt[a[i]]++;</span><br><span class="line">        mx=<span class="built_in">max</span>(mx,a[i]);</span><br><span class="line">        sum+=a[i];</span><br><span class="line">        sum%=p;</span><br><span class="line">    &#125;</span><br><span class="line">    n=mx;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        <span class="comment">//两个因数都是i的倍数</span></span><br><span class="line">        <span class="comment">//枚举i的倍数为j</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=i;j&lt;=n;j+=i)&#123;</span><br><span class="line">            F[i]+=cnt[j]*j;</span><br><span class="line">            F[i]%=p;</span><br><span class="line">        &#125;</span><br><span class="line">        F[i]*=F[i];</span><br><span class="line">        F[i]%=p;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span> ans=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,n)&#123;</span><br><span class="line">        <span class="comment">//公约数为i的方案数，计算反演式</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> d=i;d&lt;=n;d+=i)&#123;</span><br><span class="line">            <span class="comment">//mu[d/i]可能为负</span></span><br><span class="line">            f[i]=(f[i]+F[d]*mu[d/i]%p+p)%p;</span><br><span class="line">        &#125;</span><br><span class="line">        ans=(ans+f[i]*inv[i]%p+p)%p;</span><br><span class="line">    &#125;</span><br><span class="line">    ans=(ans-sum+p)%p;</span><br><span class="line">    ans=ans*inv[<span class="number">2</span>]%p;</span><br><span class="line">    cout&lt;&lt;ans&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="分块优化"><a href="#分块优化" class="headerlink" title="分块优化"></a>分块优化</h4><p><a href="https://www.luogu.com.cn/problem/P3455">ZAP Queries</a></p><p>$\sum\sum[gcd(i,j)&#x3D;d]$ 显然是非常好计算的，但是在多case的情况下，如果不进行优化很容易超时</p><p>优化前</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i=d;i&lt;=<span class="built_in">min</span>(a,b);i+=d)&#123;</span><br><span class="line">    F[i]=(a/i)*(b/i);</span><br><span class="line">    ans+=F[i]*mu[i/d];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>发现F[i]其实可以分块来做，在一定区间内，F[i]的值是不变的，统计莫比乌斯函数区间和即可</p><p>$f(d)&#x3D;\sum_{d|d’}F(d’)*\mu(\frac{d’}{n})&#x3D;\sum_{d|d’}\mu(\frac{d’}{d})\frac{a}{d’}\frac{b}{d’}$</p><p>$t&#x3D;\frac{d’}{d};;ans&#x3D;\sum_{t}\mu(t)\frac{a}{dt}\frac{b}{dt}$ t时枚举量，实际上除数是a&#x2F;d,b&#x2F;d</p><p>$&#x3D;\sum_{t}\mu(t)\frac{\frac{a}{d}}{t}\frac{\frac{d}{d}}{t}$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="type">int</span> l=<span class="number">1</span>,r;l&lt;=<span class="built_in">min</span>(a/d,b/d);l=r<span class="number">+1</span>)&#123;</span><br><span class="line">    r=<span class="built_in">min</span>(a/d/(a/d/l),b/d/(b/d/l));</span><br><span class="line">    ans+=(a/(l*d))*(b/(l*d))*(mu[r]-mu[l<span class="number">-1</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a/=d,b/=d;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> l=<span class="number">1</span>,r;l&lt;=<span class="built_in">min</span>(a,b);l=r<span class="number">+1</span>)&#123;</span><br><span class="line">    <span class="type">int</span> r=<span class="built_in">min</span>(a/(a/l),b/(b/l));</span><br><span class="line">    ans+=(a/l)*(b/l)*(sum[r]-sum[l<span class="number">-1</span>]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://www.luogu.com.cn/problem/P3312">数表</a></p><p>$\sum_{i&#x3D;1}^n\sum_{j&#x3D;1}^m[\varphi(\gcd(i,j))&lt;&#x3D;a] \quad \varphi(n)&#x3D;\sum_{d|n}d$</p><p>$\sum_{\varphi(d)\le a}\varphi(d)f(d);;,f(d)&#x3D;\sum_{n|d}F(d)\mu(\frac{d}{n})$</p><p>$\varphi(d’)\sum_{d’|d}\mu(\frac{d}{n})\frac{n}{d}\frac{n}{d}$ 不需要确保枚举量是d的倍数，使用树状数组求前面一部分前缀和，只有d的倍数才会被添加到树状数组中</p><p>离线查找答案</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//记录1e5以下的数据和约数和</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">node</span>&#123;</span><br><span class="line">    <span class="type">int</span> sum,val;</span><br><span class="line">    <span class="type">bool</span> <span class="keyword">operator</span>&lt;(<span class="type">const</span> node&amp; o)<span class="type">const</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> sum&lt;o.sum;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;b[N];</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">QUERY</span>&#123;</span><br><span class="line">    <span class="type">int</span> n,m,a,id;</span><br><span class="line">    <span class="type">bool</span> <span class="keyword">operator</span>&lt;(<span class="type">const</span> QUERY&amp; o)<span class="type">const</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> a&lt;o.a;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;Q[N];</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">solve2</span><span class="params">(<span class="type">int</span> n,<span class="type">int</span> m)</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> t=<span class="built_in">min</span>(n,m),ans=<span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> l=<span class="number">1</span>,r;l&lt;=t;l=r<span class="number">+1</span>)&#123;</span><br><span class="line">        r=<span class="built_in">min</span>(n/(n/l),m/(m/l));</span><br><span class="line">        <span class="comment">//query只会查到[l,r]区间内d的倍数的位置</span></span><br><span class="line">        ans+=(<span class="built_in">query</span>(r)-<span class="built_in">query</span>(l<span class="number">-1</span>))*(n/l)*(m/l);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> ans[N];</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="comment">// de(&quot;\n\n&quot;);</span></span><br><span class="line">    <span class="type">int</span> q;</span><br><span class="line">    cin&gt;&gt;q;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,q)&#123;</span><br><span class="line">        cin&gt;&gt;Q[i].n&gt;&gt;Q[i].m&gt;&gt;Q[i].a;</span><br><span class="line">        Q[i].id=i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">sort</span>(b<span class="number">+1</span>,b<span class="number">+100001</span>);</span><br><span class="line">    <span class="built_in">sort</span>(Q<span class="number">+1</span>,Q<span class="number">+1</span>+q);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">1</span>,j=<span class="number">1</span>;i&lt;=q;i++)&#123;</span><br><span class="line">        <span class="keyword">while</span>(b[j].sum&lt;=Q[i].a <span class="keyword">and</span> j&lt;=<span class="number">1e5</span>)&#123;</span><br><span class="line">            <span class="comment">//将满足条件的约数和的倍数塞入到树状数组中</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="type">int</span> d=b[j].val;d&lt;=<span class="number">1e5</span>;d+=b[j].val)&#123;</span><br><span class="line">                <span class="built_in">add</span>(d,b[j].sum*mu[d/b[j].val]);  </span><br><span class="line">            &#125;</span><br><span class="line">            j++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//分块查找</span></span><br><span class="line">        ans[Q[i].id]=<span class="built_in">solve2</span>(Q[i].n,Q[i].m);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">1</span>,q)&#123;</span><br><span class="line">        <span class="keyword">if</span>(ans[i]&lt;<span class="number">0</span>) ans[i]+=<span class="built_in">pow</span>(<span class="number">2</span>,<span class="number">31</span>);</span><br><span class="line">        cout&lt;&lt;ans[i]&lt;&lt;endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 程序设计算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数论 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>筛法和积性函数</title>
      <link href="/2025/04/26/%E7%AD%9B%E6%B3%95%E5%92%8C%E7%A7%AF%E6%80%A7%E5%87%BD%E6%95%B0/"/>
      <url>/2025/04/26/%E7%AD%9B%E6%B3%95%E5%92%8C%E7%A7%AF%E6%80%A7%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<h4 id="线性筛筛质数"><a href="#线性筛筛质数" class="headerlink" title="线性筛筛质数"></a>线性筛筛质数</h4><p>线性筛为什么可以做到线性，它让每个合数只被<strong>最小的质因数</strong>筛选一次而不发生重复筛选</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">rep</span>(i,<span class="number">2</span>,n)&#123;</span><br><span class="line">    <span class="keyword">if</span>(!vis[i]) pri[++cnt]=i;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=cnt <span class="keyword">and</span> pri[j]*i&lt;=<span class="number">1e8</span>;j++)&#123;</span><br><span class="line">        <span class="type">int</span> m=i*pri[j];</span><br><span class="line">        vis[m]=<span class="number">1</span>;</span><br><span class="line">        <span class="comment">//说明pri[j]是i的最小质因数，同样是被筛数m的最小质因数，所以再后面的数就应该由pri[j]筛掉，而不是j++</span></span><br><span class="line">        <span class="comment">//那么6*3=18应该等到i=9是被pri[j]=2筛掉，所以i=6时遇到pri[j]=2就可以停止了</span></span><br><span class="line">        <span class="keyword">if</span>(i%pri[j]==<span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="线性筛求约数个数"><a href="#线性筛求约数个数" class="headerlink" title="线性筛求约数个数"></a>线性筛求约数个数</h4><p>$n&#x3D;\prod_{i&#x3D;1}^sp_i^{\alpha_i}\qquad ans&#x3D;\prod_{i&#x3D;1}^s(\alpha_i+1)$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">    a[<span class="number">1</span>]=<span class="number">1</span>,d[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line">    <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">2</span>,<span class="number">1e7</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!vis[i])&#123;</span><br><span class="line">            pri[++cnt]=i;</span><br><span class="line">            a[i]=<span class="number">1</span>;d[i]=<span class="number">2</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=cnt <span class="keyword">and</span> i*pri[j]&lt;=<span class="number">1e7</span>;j++)&#123;</span><br><span class="line">            <span class="type">int</span> m=i*pri[j];</span><br><span class="line">            vis[m]=<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%pri[j]==<span class="number">0</span>)&#123;</span><br><span class="line">                a[m]=a[i]<span class="number">+1</span>;</span><br><span class="line">                d[m]=d[i]/a[m]*(a[m]<span class="number">+1</span>);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                a[m]=<span class="number">1</span>;</span><br><span class="line">                d[m]=d[i]*<span class="number">2</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="线性筛求约数和"><a href="#线性筛求约数和" class="headerlink" title="线性筛求约数和"></a>线性筛求约数和</h4><p>$n&#x3D;\prod_{i&#x3D;1}^sp_i^{\alpha_i}\qquad (n)&#x3D;\prod_{i&#x3D;1}^s\sum_{j&#x3D;0}^{\alpha_i}p_i^j$</p><blockquote><p>a[i]是最小质因数的约数和</p><p>d[i]是函数值</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">2</span>,<span class="number">1e7</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!vis[i])&#123;</span><br><span class="line">            pri[++cnt]=i;</span><br><span class="line">            a[i]=d[i]=i<span class="number">+1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=cnt <span class="keyword">and</span> i*pri[j]&lt;=<span class="number">1e7</span>;j++)&#123;</span><br><span class="line">            <span class="type">int</span> m=i*pri[j];</span><br><span class="line">            vis[m]=<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%pri[j]==<span class="number">0</span>)&#123;</span><br><span class="line">                a[m]=a[i]*p[j]<span class="number">+1</span>;</span><br><span class="line">                d[m]=d[i]/a[i]*a[m]</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                a[m]=p[j]<span class="number">+1</span>;</span><br><span class="line">                d[m]=d[i]*a[m];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="线性筛求莫比乌斯函数"><a href="#线性筛求莫比乌斯函数" class="headerlink" title="线性筛求莫比乌斯函数"></a>线性筛求莫比乌斯函数</h4><p>莫比乌斯函数：</p><p>包含相同质因子为0</p><p>不同质因数为偶数个为1，奇数为-1</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">2</span>,<span class="number">1e7</span>)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!vis[i])&#123;</span><br><span class="line">            pri[++cnt]=i;</span><br><span class="line">            mu[i]=<span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=cnt <span class="keyword">and</span> i*pri[j]&lt;=<span class="number">1e7</span>;j++)&#123;</span><br><span class="line">            <span class="type">int</span> m=i*pri[j];</span><br><span class="line">            vis[m]=<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">if</span>(i%pri[j]==<span class="number">0</span>)&#123;</span><br><span class="line">                mu[m]=<span class="number">0</span>;s</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//比i多了一个质因数，奇偶互换0不变，所以取反</span></span><br><span class="line">                mu[m]=-mu[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="莫比乌斯反演"><a href="#莫比乌斯反演" class="headerlink" title="莫比乌斯反演"></a>莫比乌斯反演</h5><p>设任意算术函数f, 其和函数为 $F(n)&#x3D;\sum_{d|n}f(d)$ 即n的所有正因数的函数和称为和函数</p><p>莫比乌斯函数就是&#x3D;&#x3D;和函数到原函数的反向过程&#x3D;&#x3D; $f(n)&#x3D;\sum_{d|n}\mu(d)F(\frac{n}{d})$ </p><h4 id="线性筛与积性函数"><a href="#线性筛与积性函数" class="headerlink" title="线性筛与积性函数"></a>线性筛与积性函数</h4><p>其实上诉问题，因子和$(n)&#x3D;\sum_{d|n}d$ 和约数个数 $(n)&#x3D;\sum_{d|n}1$都是积性函数</p><p>积性函数？对于互为质数的正整数p,q有 $(p<em>q)&#x3D;(p)</em>(q)$</p><p>&#x3D;&#x3D;积性函数都可以用线性筛求解1~n的所有函数值&#x3D;&#x3D;</p><p>O(n)求1~n的欧拉函数是一个比较简单但直观的例子，所以线性筛也被称为欧拉筛</p><p>欧拉函数</p><p>$n&#x3D;\prod_{i&#x3D;1}^sp_i^{\alpha_i} \quad (n)&#x3D;n\prod(1-\frac{1}{p_i})$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">solve</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line">    <span class="built_in">rep</span>(i,<span class="number">2</span>,n)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!vis[i])&#123;</span><br><span class="line">            phi[i]=i<span class="number">-1</span>;</span><br><span class="line">            <span class="comment">//单独考虑质数的情况</span></span><br><span class="line">            pr[++cnt]=i;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=cnt <span class="keyword">and</span> pr[j]&lt;=n/i;j++)&#123;</span><br><span class="line">            <span class="type">int</span> m=i*pr[j];</span><br><span class="line">            vis[m]=<span class="number">1</span>;</span><br><span class="line">            <span class="comment">//需要特别判断pq不互质的情况的</span></span><br><span class="line">            <span class="keyword">if</span>(i%pr[j]==<span class="number">0</span>)&#123;</span><br><span class="line">                <span class="comment">//没有出现新的质因数p，就扩大到m就行</span></span><br><span class="line">                phi[m]=phi[i]*pr[j];</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">//出现了新的质因数</span></span><br><span class="line">                phi[m]=phi[i]*phi[pr[j]];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><a href="https://ac.nowcoder.com/acm/contest/22769/B">完全积性函数 华华给月月出题</a></p><blockquote><p>幂函数对于任何的p,q都满足$(p<em>q)&#x3D;p</em>q$ 被称为完全积性函数，不需要单独考虑遍历值不为最小质因数的情况</p></blockquote><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cin&gt;&gt;n;</span><br><span class="line">f[<span class="number">1</span>]=<span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> cnt=<span class="number">0</span>;</span><br><span class="line"><span class="built_in">rep</span>(i,<span class="number">2</span>,n)&#123;</span><br><span class="line">    <span class="keyword">if</span>(!vis[i])&#123;</span><br><span class="line">        <span class="comment">//单独考虑一下质数就行</span></span><br><span class="line">        pri[++cnt]=i;</span><br><span class="line">        f[i]=<span class="built_in">qpow</span>(i,n,p)%p;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> j=<span class="number">1</span>;j&lt;=cnt <span class="keyword">and</span> pri[j]&lt;=n/i;j++)&#123;</span><br><span class="line">        vis[i*pri[j]]=<span class="number">1</span>;</span><br><span class="line">        <span class="comment">//通过乘性关系递推</span></span><br><span class="line">        f[i*pri[j]]=f[i]*f[pri[j]]%p;</span><br><span class="line">        <span class="keyword">if</span>(i%pri[j]==<span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 程序设计算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数论 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
